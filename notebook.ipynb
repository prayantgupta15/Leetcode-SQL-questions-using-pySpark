{"cells":[{"source":"1. ### **-- 2346. Compute the Rank as a Percentage**","metadata":{},"cell_type":"markdown","id":"3fbef007-9bd4-4a3b-a844-d1deeaf80c2b"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum, when, abs, round, lit,dense_rank,count\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nspark = SparkSession.builder.appName('eer').createOrReplace()\nprint(spark.sparkContext.getConf().getAll())\n\n\nstud_schema = StructType([\n    StructField('student_id',IntegerType()),\n    StructField('department_id',IntegerType()),\n    StructField('mark',IntegerType())\n])\n\nstud_data = [\n[2          , 2             ,650],\n[ 8          , 2             , 650],\n[ 7          , 1             , 920],\n[ 1          , 1             , 610],\n[ 3          , 1             , 530]\n ]\n\nstud_df = spark.createDataFrame(schema=stud_schema,data=stud_data)\nWindow.partitionBy(col('department_id'))\nwd = Window.partitionBy(col('department_id')).orderBy(col('mark').desc())\ncwd = Window.partitionBy(col('department_id'))\nans = stud_df.withColumn('perc',\n                   (dense_rank().over(wd) -1)*100/(count('*').over(cwd)-1)\n                  ).withColumn('rnk',dense_rank().over(wd))\\\n.withColumn('cnt',count(\"*\").over(cwd))\n\nans.show()","metadata":{"executionCancelledAt":null,"executionTime":41,"lastExecutedAt":1736345672892,"lastExecutedByKernel":"c65a5483-5354-44a7-a8e6-f3e09e3f7f78","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum, when, abs, round, lit,dense_rank,count\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nspark = SparkSession.builder.appName('eer').createOrReplace()\nprint(spark.sparkContext.getConf().getAll())\n\n\nstud_schema = StructType([\n    StructField('student_id',IntegerType()),\n    StructField('department_id',IntegerType()),\n    StructField('mark',IntegerType())\n])\n\nstud_data = [\n[2          , 2             ,650],\n[ 8          , 2             , 650],\n[ 7          , 1             , 920],\n[ 1          , 1             , 610],\n[ 3          , 1             , 530]\n ]\n\nstud_df = spark.createDataFrame(schema=stud_schema,data=stud_data)\nWindow.partitionBy(col('department_id'))\nwd = Window.partitionBy(col('department_id')).orderBy(col('mark').desc())\ncwd = Window.partitionBy(col('department_id'))\nans = stud_df.withColumn('perc',\n                   (dense_rank().over(wd) -1)*100/(count('*').over(cwd)-1)\n                  ).withColumn('rnk',dense_rank().over(wd))\\\n.withColumn('cnt',count(\"*\").over(cwd))\n\nans.show()","outputsMetadata":{"0":{"height":269,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":227,"type":"stream"}}},"cell_type":"code","id":"e7be043e-ef9d-481c-ab9e-649498c11d10","outputs":[{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Window\n\u001b[1;32m      6\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m----> 7\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateOrReplace\u001b[49m()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mgetConf()\u001b[38;5;241m.\u001b[39mgetAll())\n\u001b[1;32m     11\u001b[0m stud_schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m     12\u001b[0m     StructField(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudent_id\u001b[39m\u001b[38;5;124m'\u001b[39m,IntegerType()),\n\u001b[1;32m     13\u001b[0m     StructField(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepartment_id\u001b[39m\u001b[38;5;124m'\u001b[39m,IntegerType()),\n\u001b[1;32m     14\u001b[0m     StructField(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmark\u001b[39m\u001b[38;5;124m'\u001b[39m,IntegerType())\n\u001b[1;32m     15\u001b[0m ])\n","\u001b[0;31mAttributeError\u001b[0m: 'Builder' object has no attribute 'createOrReplace'"],"ename":"AttributeError","evalue":"'Builder' object has no attribute 'createOrReplace'"}],"execution_count":6},{"source":"2. ### **2783. Flight Occupancy and Waitlist Analysis**","metadata":{},"cell_type":"markdown","id":"71ce4dfa-6067-4f53-8f62-753933231945"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum, when, abs, round, lit,dense_rank,count\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\n\nFlights_schema = StructType(\n   [\n    StructField('flight_id',IntegerType()),\n    StructField('capacity',IntegerType())\n   ]\n)\n\nPassengers_schema = StructType([\n    StructField('passenger_id',IntegerType()),\n    StructField('flight_id',IntegerType()),\n])\n\n\nFlights_data= [\n    [1       ,   2        ],\n    [ 2     ,     2        ],\n    [ 3      ,    1 ]\n ]\n\nPassengers_data = [\n[ 101          ,1         ],\n[102          ,1         ],\n[103          ,1         ],\n[104          ,2         ],\n[105          ,2         ],\n[106          ,3         ],\n[107          ,3  ]\n]\n\n\nflights_df = spark.createDataFrame(schema=Flights_schema,data=Flights_data)\nPassengers_df = spark.createDataFrame(schema=Passengers_schema,data=Passengers_data)\n\n\nbookings_df = Passengers_df.groupBy(col('flight_id')).agg(count('*').alias('booked_cnt'))\n\n\nans = flights_df.join(bookings_df,'flight_id','left')\\\n.withColumn('waitlist_cnt',col('booked_cnt')-col('capacity'))\n\n\nans.show()\n\n\n","metadata":{"executionCancelledAt":null,"executionTime":1327,"lastExecutedAt":1720809694333,"lastExecutedByKernel":"ecc5317a-410c-41ce-8ae4-e49295160ce6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum, when, abs, round, lit,dense_rank,count\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\n\nFlights_schema = StructType(\n   [\n    StructField('flight_id',IntegerType()),\n    StructField('capacity',IntegerType())\n   ]\n)\n\nPassengers_schema = StructType([\n    StructField('passenger_id',IntegerType()),\n    StructField('flight_id',IntegerType()),\n])\n\n\nFlights_data= [\n    [1       ,   2        ],\n    [ 2     ,     2        ],\n    [ 3      ,    1 ]\n ]\n\nPassengers_data = [\n[ 101          ,1         ],\n[102          ,1         ],\n[103          ,1         ],\n[104          ,2         ],\n[105          ,2         ],\n[106          ,3         ],\n[107          ,3  ]\n]\n\n\nflights_df = spark.createDataFrame(schema=Flights_schema,data=Flights_data)\nPassengers_df = spark.createDataFrame(schema=Passengers_schema,data=Passengers_data)\n\n\nbookings_df = Passengers_df.groupBy(col('flight_id')).agg(count('*').alias('booked_cnt'))\n\n\nans = flights_df.join(bookings_df,'flight_id','left')\\\n.withColumn('waitlist_cnt',col('booked_cnt')-col('capacity'))\n\n\nans.show()\n\n\n","outputsMetadata":{"0":{"height":185,"type":"stream"}}},"cell_type":"code","id":"d36c0bf0-c093-4685-add0-bb9a5559b77e","outputs":[{"output_type":"stream","name":"stdout","text":"+---------+--------+----------+------------+\n|flight_id|capacity|booked_cnt|waitlist_cnt|\n+---------+--------+----------+------------+\n|        1|       2|         3|           1|\n|        3|       1|         2|           1|\n|        2|       2|         2|           0|\n+---------+--------+----------+------------+\n\n"}],"execution_count":2},{"source":"### **3. 550- Game Play Analysis IV**","metadata":{},"cell_type":"markdown","id":"eedc87e6-5093-44b6-b62f-5faa5c991159"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum, when, abs, round, lit,dense_rank,count,min,datediff\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\n\n\nactivity_schema = StructType(\n[\n    StructField('player_id',IntegerType()),\n    StructField('device_id',IntegerType()),\n    StructField('event_date',DateType()),\n    StructField('games_played',IntegerType())\n])\n\nactivity_data = [\n[ 1,          2,          datetime.strptime('2016-03-01','%Y-%m-%d'),  5            ],\n[ 1,          2,          datetime.strptime('2016-03-02','%Y-%m-%d'),  6            ],\n[ 2,          3,          datetime.strptime('2017-06-25','%Y-%m-%d'),  1            ],\n[ 3,          1,          datetime.strptime('2016-03-02','%Y-%m-%d'),  0            ],\n[ 3,          4,          datetime.strptime('2018-07-03','%Y-%m-%d'),  5            ],\n[ 3,          3,          datetime.strptime('2016-03-03','%Y-%m-%d'),  2            ]\n]\n\nactivity_df = spark.createDataFrame(schema=activity_schema,data=activity_data)\nactivity_df.show()\n\nactivity_df.withColumn('diff',\n                      datediff(\n                        col('event_date'),\n                        min('event_date').over(Window.partitionBy('player_id').orderBy('event_date')) \n                      ))\\\n.where(col('diff')==1).select('player_id').show()","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":395,"type":"stream"},"1":{"height":227,"type":"stream"}}},"cell_type":"code","id":"4cd3f2ce-6b28-442b-b89b-e6874723f451","outputs":[{"output_type":"stream","name":"stdout","text":"+---------+---------+----------+------------+\n|player_id|device_id|event_date|games_played|\n+---------+---------+----------+------------+\n|        1|        2|2016-03-01|           5|\n|        1|        2|2016-03-02|           6|\n|        2|        3|2017-06-25|           1|\n|        3|        1|2016-03-02|           0|\n|        3|        4|2018-07-03|           5|\n|        3|        3|2016-03-03|           2|\n+---------+---------+----------+------------+\n\n+---------+\n|player_id|\n+---------+\n|        1|\n|        3|\n+---------+\n\n"}],"execution_count":13},{"source":"### **1454 - Active Users**","metadata":{},"cell_type":"markdown","id":"3c629a9b-5344-4ded-b9a1-94e844a3f8aa"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import datediff,lag,ifnull,curdate,count\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\n\n\nlogins_schema = StructType(\n[\n    StructField('id',IntegerType()),\n    StructField('login_date',DateType()),\n])\n\ndata = [\n[ 7  ,datetime.strptime('2020-05-30','%Y-%m-%d')],\n[ 1  ,datetime.strptime('2020-05-30','%Y-%m-%d')],\n    [ 1  ,datetime.strptime('2020-05-30','%Y-%m-%d')],\n    [ 1  ,datetime.strptime('2020-05-30','%Y-%m-%d')],\n    [ 1  ,datetime.strptime('2020-05-30','%Y-%m-%d')],\n    [ 1  ,datetime.strptime('2020-05-30','%Y-%m-%d')],\n[ 7  ,datetime.strptime('2020-05-31','%Y-%m-%d')],\n[ 7  ,datetime.strptime('2020-06-01','%Y-%m-%d')],\n[ 7  ,datetime.strptime('2020-06-02','%Y-%m-%d')],\n[ 7  ,datetime.strptime('2020-06-02','%Y-%m-%d')],\n[ 7  ,datetime.strptime('2020-06-03','%Y-%m-%d')],\n[ 1  ,datetime.strptime('2020-06-07','%Y-%m-%d')],\n[ 7  ,datetime.strptime('2020-06-10','%Y-%m-%d')]\n]\n\n\nlogin_df = spark.createDataFrame(schema=logins_schema,data=data)\nlogin_df.show()\n\n# 5 consecutive days\n\n\n# method-1\n\n# login_df.distinct('login_date').show()\n\nlogin_df.dropDuplicates()\\\n.withColumn('flag',\n            ifnull(\n            (datediff('login_date',lag('login_date',1,'login_date').over(Window.partitionBy('id').\n                                                                   orderBy('login_date'))\n               )==1) & \n       (datediff('login_date',lag('login_date',2,'login_date').over(Window.partitionBy('id').\n                                                                   orderBy('login_date'))\n               )==2) & \n       (datediff('login_date',lag('login_date',3,'login_date').over(Window.partitionBy('id').\n                                                                   orderBy('login_date'))\n               )==3) & \n       (datediff('login_date',lag('login_date',4,'login_date').over(Window.partitionBy('id').\n                                                                   orderBy('login_date'))\n               )==4),\n                lit(False)\n            )\n                   )\\\n.show()\n# .where(col('flag')==True)\\\n# .fillna(col('flag'),'False')\\\n\n\n# method-2\n\nlogin_df.dropDuplicates().withColumn('rnk',\ndatediff(curdate(),'login_date')+dense_rank().over(Window.partitionBy('id').orderBy('login_date'))\n                   ).groupby('id','rnk').agg(count('*').alias('cnt')).where(col('cnt')>4).show()","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":616,"type":"stream"},"1":{"height":311,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"cff48ddf-88fc-478c-a51a-6755006873ba","outputs":[{"output_type":"stream","name":"stdout","text":"+---+----------+\n| id|login_date|\n+---+----------+\n|  7|2020-05-30|\n|  1|2020-05-30|\n|  1|2020-05-30|\n|  1|2020-05-30|\n|  1|2020-05-30|\n|  1|2020-05-30|\n|  7|2020-05-31|\n|  7|2020-06-01|\n|  7|2020-06-02|\n|  7|2020-06-02|\n|  7|2020-06-03|\n|  1|2020-06-07|\n|  7|2020-06-10|\n+---+----------+\n\n+---+----------+-----+\n| id|login_date| flag|\n+---+----------+-----+\n|  1|2020-05-30|false|\n|  1|2020-06-07|false|\n|  7|2020-05-30|false|\n|  7|2020-05-31|false|\n|  7|2020-06-01|false|\n|  7|2020-06-02|false|\n|  7|2020-06-03| true|\n|  7|2020-06-10|false|\n+---+----------+-----+\n\n+---+----+---+\n| id| rnk|cnt|\n+---+----+---+\n|  7|1508|  5|\n+---+----+---+\n\n"}],"execution_count":41},{"source":"### **2292 - Products With Three or More Orders in Two Consecutive Years**","metadata":{},"cell_type":"markdown","id":"0060f437-004d-4406-8bc0-812f099b7bb9"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import datediff,lag,ifnull,curdate,count,date_format,col\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\norders_schema = StructType(\n[\n    StructField('order_id',IntegerType()),\n    StructField('product_id',IntegerType()),\n    StructField('purchase_date',DateType())\n]\n)\n\norders_data = [\n[1,1,datetime.strptime('2020-03-16','%Y-%m-%d')],\n[2,1,datetime.strptime('2020-12-02','%Y-%m-%d')],\n[3,1,datetime.strptime('2020-05-10','%Y-%m-%d')],\n[4,1,datetime.strptime('2021-12-23','%Y-%m-%d')],\n[5,1,datetime.strptime('2021-05-21','%Y-%m-%d')],\n[6,1,datetime.strptime('2021-10-11','%Y-%m-%d')],\n[7,2,datetime.strptime('2022-10-11','%Y-%m-%d')],\n[8,2,datetime.strptime('2023-10-11','%Y-%m-%d')]\n]\n\n\norder_df = spark.createDataFrame(schema=orders_schema,data=orders_data)\n\ncte = order_df.groupby(date_format('purchase_date','y').alias('yr'),'product_id').agg(count('*')\\\n                                                                                      .alias('cnt'))\n\ncte.show()\ncte.printSchema()\n# cte.where(col('cnt')>=2).show()\n\ncte1= cte.alias(\"cte1\")\ncte2= cte.alias(\"cte2\")\n\n# .where(col('cnt')>=2)\n# .where(col('cnt')>=2)\n\ncte1.join(\n    (cte2),\n          (col(\"cte1.product_id\") == col(\"cte2.product_id\")) &\n          (col(\"cte1.yr\")+1 == col(\"cte2.yr\"))&\n                (col('cte1.cnt')>=2)&(col('cte2.cnt')>=2)\n        ).show()","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":437,"type":"stream"},"1":{"height":185,"type":"stream"}}},"cell_type":"code","id":"66013c9e-5432-4b98-9c89-34f503bf0267","outputs":[{"output_type":"stream","name":"stdout","text":"+----+----------+---+\n|  yr|product_id|cnt|\n+----+----------+---+\n|2021|         1|  3|\n|2020|         1|  3|\n|2023|         2|  1|\n|2022|         2|  1|\n+----+----------+---+\n\nroot\n |-- yr: string (nullable = true)\n |-- product_id: integer (nullable = true)\n |-- cnt: long (nullable = false)\n\n+----+----------+---+----+----------+---+\n|  yr|product_id|cnt|  yr|product_id|cnt|\n+----+----------+---+----+----------+---+\n|2020|         1|  3|2021|         1|  3|\n+----+----------+---+----+----------+---+\n\n"}],"execution_count":94},{"source":"### **2922 - Market Analysis III**","metadata":{},"cell_type":"markdown","id":"750fc456-bb10-4a19-aee1-43fef35f866d"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import datediff,lag,ifnull,curdate,count,date_format,col\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\nusers_sc = StructType([\n    StructField('seller_id',IntegerType()),\n    StructField('favorite_brand',StringType())    \n])\n\nitems_sc = StructType([\n    StructField('item_id',IntegerType()),\n    StructField('item_brand',StringType())\n])\n\norders_sc = StructType(\n[\n    StructField('orderId',IntegerType()),\n    StructField('item_id',IntegerType()),\n    StructField('seller_id',IntegerType())\n]\n)\n\n\nusers_data = [\n[1       ,'Lenovo'],\n[ 2         ,'Samsung'],\n[ 3         ,'LG']\n]\n\nitems_data = [\n    [1       ,'Samsung'],\n[ 2       ,'Lenovo'],\n[ 3       ,'LG'],\n[4       ,'HP']\n]\n\n\norders_data = [\n[1,4       ,2],\n[ 2,2       , 3],\n[ 3,3       , 3],\n[ 4,1       , 2],\n[ 5,4       , 2]   \n]\n\n\nusers_df = spark.createDataFrame(schema=users_sc,data=users_data)\nitems_df = spark.createDataFrame(schema=items_sc,data=items_data)\norders_df = spark.createDataFrame(schema=orders_sc,data=orders_data)\n\norders_df.join(items_df,'item_id').join(users_df,'seller_id').where(col('item_brand')!=col('favorite_brand'))\\\n.drop_duplicates(['seller_id','item_id'])\\\n.groupby(col('seller_id')).agg(count('item_id').alias('num_items')).show()","metadata":{"executionCancelledAt":null,"executionTime":1355,"lastExecutedAt":1721811574679,"lastExecutedByKernel":"82b47b95-3b50-4c2c-aabd-8891af063f38","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import datediff,lag,ifnull,curdate,count,date_format,col\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\nusers_sc = StructType([\n    StructField('seller_id',IntegerType()),\n    StructField('favorite_brand',StringType())    \n])\n\nitems_sc = StructType([\n    StructField('item_id',IntegerType()),\n    StructField('item_brand',StringType())\n])\n\norders_sc = StructType(\n[\n    StructField('orderId',IntegerType()),\n    StructField('item_id',IntegerType()),\n    StructField('seller_id',IntegerType())\n]\n)\n\n\nusers_data = [\n[1       ,'Lenovo'],\n[ 2         ,'Samsung'],\n[ 3         ,'LG']\n]\n\nitems_data = [\n    [1       ,'Samsung'],\n[ 2       ,'Lenovo'],\n[ 3       ,'LG'],\n[4       ,'HP']\n]\n\n\norders_data = [\n[1,4       ,2],\n[ 2,2       , 3],\n[ 3,3       , 3],\n[ 4,1       , 2],\n[ 5,4       , 2]   \n]\n\n\nusers_df = spark.createDataFrame(schema=users_sc,data=users_data)\nitems_df = spark.createDataFrame(schema=items_sc,data=items_data)\norders_df = spark.createDataFrame(schema=orders_sc,data=orders_data)\n\norders_df.join(items_df,'item_id').join(users_df,'seller_id').where(col('item_brand')!=col('favorite_brand'))\\\n.drop_duplicates(['seller_id','item_id'])\\\n.groupby(col('seller_id')).agg(count('item_id').alias('num_items')).show()\n\n\n\n\n","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":164,"type":"stream"}}},"cell_type":"code","id":"d2eb179a-8db1-4fd3-b248-652c70ebb3c9","outputs":[{"output_type":"stream","name":"stderr","text":"                                                                                \r"},{"output_type":"stream","name":"stdout","text":"+---------+---------+\n|seller_id|num_items|\n+---------+---------+\n|        3|        1|\n|        2|        1|\n+---------+---------+\n\n"}],"execution_count":5},{"source":"### **1341 - Movie Rating**","metadata":{},"cell_type":"markdown","id":"f4efc9b0-6ead-487b-95de-7a5be91a8442"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max,datediff,lag,ifnull,curdate,count,date_format,col,avg\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\nusers_sc = StructType([\n    StructField('user_id',IntegerType()),\n    StructField('name',StringType())    \n])\n\nusers_data =[\n[  1           ,'Daniel'],\n[ 2            ,'Monica'],\n[3           ,'Maria'],\n[ 4           ,'James']\n]\n\nmovieRating_sc = StructType([\n    StructField('movie_id',IntegerType()),\n    StructField('user_id',IntegerType()),    \n    StructField('rating',IntegerType()),\n    StructField('created_at',DateType()),\n])\nmovieRating_data = [\n[1           , 1            ,3              ,datetime.strptime('2020-01-12','%Y-%m-%d')],\n[ 1           , 2            , 4            ,datetime.strptime('2020-02-11','%Y-%m-%d')],\n[ 1           , 3            , 2            ,datetime.strptime('2020-02-12','%Y-%m-%d')],\n[ 1           , 4            , 1            ,datetime.strptime('2020-01-01','%Y-%m-%d')],\n[ 2           , 1            , 5            ,datetime.strptime('2020-02-17','%Y-%m-%d')], \n[ 2           , 2            , 2            ,datetime.strptime('2020-02-01','%Y-%m-%d')], \n[ 2           , 3            , 2            ,datetime.strptime('2020-03-01','%Y-%m-%d')],\n[ 3           , 1            , 3            ,datetime.strptime('2020-02-22','%Y-%m-%d')],  \n[ 3           , 2            , 4            ,datetime.strptime('2020-02-25','%Y-%m-%d')]\n]\n    \nusers_df = spark.createDataFrame(schema=users_sc,data=users_data)    \nmovieRating_df = spark.createDataFrame(schema=movieRating_sc,data=movieRating_data)    \n#     user with max ratings:\ncntDF= movieRating_df.groupby(col('user_id')).agg(count('movie_id').alias('cnt')).join(users_df,'user_id').orderBy(col('cnt').desc(),col('name')).limit(1)\ncntDF.show()\n\n# how spark handles date_format\nmovieRating_df.withColumn('YYYY-mm',date_format('created_at','yyyy-M')).limit(1).show()\n\n# movie with highest ratings:\nmovieRating_df.where(date_format('created_at','yyyy-M')=='2020-2').groupBy('movie_id').agg(avg('rating').alias('avgRatings'))\\\n.orderBy(col('avgRatings').desc()).limit(1)\\\n.show()","metadata":{"executionCancelledAt":null,"executionTime":1032,"lastExecutedAt":1721827821752,"lastExecutedByKernel":"fada8d33-2e6e-4a13-a399-d7463ba7c9c1","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max,datediff,lag,ifnull,curdate,count,date_format,col,avg\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\nusers_sc = StructType([\n    StructField('user_id',IntegerType()),\n    StructField('name',StringType())    \n])\n\nusers_data =[\n[  1           ,'Daniel'],\n[ 2            ,'Monica'],\n[3           ,'Maria'],\n[ 4           ,'James']\n]\n\nmovieRating_sc = StructType([\n    StructField('movie_id',IntegerType()),\n    StructField('user_id',IntegerType()),    \n    StructField('rating',IntegerType()),\n    StructField('created_at',DateType()),\n])\nmovieRating_data = [\n[1           , 1            ,3              ,datetime.strptime('2020-01-12','%Y-%m-%d')],\n[ 1           , 2            , 4            ,datetime.strptime('2020-02-11','%Y-%m-%d')],\n[ 1           , 3            , 2            ,datetime.strptime('2020-02-12','%Y-%m-%d')],\n[ 1           , 4            , 1            ,datetime.strptime('2020-01-01','%Y-%m-%d')],\n[ 2           , 1            , 5            ,datetime.strptime('2020-02-17','%Y-%m-%d')], \n[ 2           , 2            , 2            ,datetime.strptime('2020-02-01','%Y-%m-%d')], \n[ 2           , 3            , 2            ,datetime.strptime('2020-03-01','%Y-%m-%d')],\n[ 3           , 1            , 3            ,datetime.strptime('2020-02-22','%Y-%m-%d')],  \n[ 3           , 2            , 4            ,datetime.strptime('2020-02-25','%Y-%m-%d')]\n]\n    \nusers_df = spark.createDataFrame(schema=users_sc,data=users_data)    \nmovieRating_df = spark.createDataFrame(schema=movieRating_sc,data=movieRating_data)    \n#     user with max ratings:\ncntDF= movieRating_df.groupby(col('user_id')).agg(count('movie_id').alias('cnt')).join(users_df,'user_id').orderBy(col('cnt').desc(),col('name')).limit(1)\ncntDF.show()\n\n# how spark handles date_format\nmovieRating_df.withColumn('YYYY-mm',date_format('created_at','yyyy-M')).limit(1).show()\n\n# movie with highest ratings:\nmovieRating_df.where(date_format('created_at','yyyy-M')=='2020-2').groupBy('movie_id').agg(avg('rating').alias('avgRatings'))\\\n.orderBy(col('avgRatings').desc()).limit(1)\\\n.show()\n\n    \n    \n\n","outputsMetadata":{"0":{"height":395,"type":"stream"}}},"cell_type":"code","id":"6549d15d-ac71-45d1-a0d3-a610280dc46c","outputs":[{"output_type":"stream","name":"stdout","text":"+-------+---+------+\n|user_id|cnt|  name|\n+-------+---+------+\n|      1|  3|Daniel|\n+-------+---+------+\n\n+--------+-------+------+----------+-------+\n|movie_id|user_id|rating|created_at|YYYY-mm|\n+--------+-------+------+----------+-------+\n|       1|      1|     3|2020-01-12| 2020-1|\n+--------+-------+------+----------+-------+\n\n+--------+----------+\n|movie_id|avgRatings|\n+--------+----------+\n|       3|       3.5|\n+--------+----------+\n\n"}],"execution_count":39},{"source":"### **-- 2688 - Find Active Users**","metadata":{},"cell_type":"markdown","id":"c6e2d17f-4856-4287-890a-1cbe5248d288"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max,datediff,lag,ifnull,curdate,count,date_format,col,avg\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\nusers_schema = StructType(\n[\n    StructField('order_id',IntegerType()),\n    StructField('user_id',IntegerType()),\n    StructField('item',StringType()),\n    StructField('created_at',DateType()),\n    StructField('amount',IntegerType())\n])\n\n\nusers_data = \n","metadata":{},"cell_type":"code","id":"5ae9ce9b-2175-43f3-a60a-919d7126c6b9","outputs":[],"execution_count":null},{"source":"### **2394 - Employees With Deductions**","metadata":{},"cell_type":"markdown","id":"bffb41b9-184d-4a83-80cd-a2a14f9aad23"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max,datediff,lag,ifnull,curdate,count,date_format,col,avg,ceil,sum,ifnull,lit,when\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType,TimestampType, LongType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\nprint(datetime.strptime('2020-02-25 09:00:00','%Y-%m-%d %H:%M:%S'))\n\nemployees_schema = StructType(\n[\n    StructField('employee_id',IntegerType()),\n    StructField('needed_hours',IntegerType())\n])\n\nlogs_schema = StructType(\n[\n    StructField('employee_id',IntegerType()),\n    StructField('in_time',TimestampType())\n    ,StructField('out_time',TimestampType())\n])\n\nemployees_data = [\n[1          , 20],\n[ 2         , 12],\n[ 3         , 2 ]\n]\n\nmyDateTime = lambda  x:datetime.strptime(x,'%Y-%m-%d %H:%M:%S')\n\nlogs_data = [\n[1,myDateTime('2022-10-01 09:00:00'),myDateTime('2022-10-01 17:00:00')],\n[1,myDateTime('2022-10-06 09:05:04'),myDateTime('2022-10-06 17:09:03')],\n[1,myDateTime('2022-10-12 23:00:00'),myDateTime('2022-10-13 03:00:01')],\n[2,myDateTime('2022-10-29 12:00:00'),myDateTime('2022-10-29 23:58:58')]\n]\n\n\n# to_timestamp\n\n\nemployees_df = spark.createDataFrame(schema=employees_schema,data=employees_data)\nlogs_df = spark.createDataFrame(schema=logs_schema,data=logs_data)\nlogs_df.show()\n\n# logs_df.groupby(col('employee_id')).agg(\n#     ceil(\n#         sum(\n#         col('out_time').cast(LongType()) -\n#         col('in_time').cast(LongType()) \n#         )/3600\n#     ).alias('diff')\n# ).show()\n\nlogs_df.groupby(col('employee_id')).agg(\n    ceil(\n        sum(\n        col('out_time').cast(LongType()) -\n        col('in_time').cast(LongType()) \n        )/3600\n    ).alias('diff')\n).join(employees_df,'employee_id','right')\\\n.where(col('needed_hours')>=when(col('diff').isNull(),0).otherwise(col('diff')))\\\n.show()\n\n# where(\n#     col('needed_hours') >= ( \n#         ifnull(col('diff'),0)\n#                            )\n#                          ).show()\n\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":374,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":206,"type":"stream"}}},"cell_type":"code","id":"19b6dd7b-f858-49af-82af-907138ee9dba","outputs":[{"output_type":"stream","name":"stdout","text":"2020-02-25 09:00:00\n+-----------+-------------------+-------------------+\n|employee_id|            in_time|           out_time|\n+-----------+-------------------+-------------------+\n|          1|2022-10-01 09:00:00|2022-10-01 17:00:00|\n|          1|2022-10-06 09:05:04|2022-10-06 17:09:03|\n|          1|2022-10-12 23:00:00|2022-10-13 03:00:01|\n|          2|2022-10-29 12:00:00|2022-10-29 23:58:58|\n+-----------+-------------------+-------------------+\n\n+-----------+----+------------+\n|employee_id|diff|needed_hours|\n+-----------+----+------------+\n|          3|NULL|           2|\n|          2|  12|          12|\n+-----------+----+------------+\n\n"}],"execution_count":30},{"source":"### **-- 578. Get Highest Answer Rate Question**","metadata":{},"cell_type":"markdown","id":"0025f5df-810a-4336-843c-52ae6407cd84"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max,datediff,lag,ifnull,curdate,count,date_format,col,avg,ceil,sum,ifnull,lit,when\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType,TimestampType, LongType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\ndata = [\n[ 5  ,'show', 285         , None      , 1      ,123],\n[ 5  ,'answer', 285         , 124124    , 1     , 124],\n[ 5  ,'show', 369         , None      , 2     , 125],\n[ 5  ,'skip', 369         , None      , 2     , 126]\n]\n\n\nsurvey_df = spark.createDataFrame(data=data,schema=\n[\n'id'      ,     \n 'action'      ,\n 'question_id' ,\n 'answer_id'    ,\n 'q_num'        ,\n 'timestamp'    ,\n])\n\nsurvey_df.groupby('question_id')\\\n.agg(\n(sum(when(col('action')=='answer',1).otherwise(0))/sum(when(col('action')=='show',1).otherwise(0))).alias('rate')\n)\\\n.orderBy(col('rate').desc()).limit(1).show()","metadata":{"executionCancelledAt":null,"executionTime":299,"lastExecutedAt":1722240709145,"lastExecutedByKernel":"7670f7d2-625a-4fee-a3bb-f77489c420d3","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max,datediff,lag,ifnull,curdate,count,date_format,col,avg,ceil,sum,ifnull,lit,when\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType,TimestampType, LongType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\ndata = [\n[ 5  ,'show', 285         , None      , 1      ,123],\n[ 5  ,'answer', 285         , 124124    , 1     , 124],\n[ 5  ,'show', 369         , None      , 2     , 125],\n[ 5  ,'skip', 369         , None      , 2     , 126]\n]\n\n\nsurvey_df = spark.createDataFrame(data=data,schema=\n[\n'id'      ,     \n 'action'      ,\n 'question_id' ,\n 'answer_id'    ,\n 'q_num'        ,\n 'timestamp'    ,\n])\n\nsurvey_df.groupby('question_id')\\\n.agg(\n(sum(when(col('action')=='answer',1).otherwise(0))/sum(when(col('action')=='show',1).otherwise(0))).alias('rate')\n)\\\n.orderBy(col('rate').desc()).limit(1).show()","outputsMetadata":{"0":{"height":143,"type":"stream"}}},"cell_type":"code","id":"35ff438d-4094-424f-bfc2-83e07361d22c","outputs":[{"output_type":"stream","name":"stdout","text":"+-----------+----+\n|question_id|rate|\n+-----------+----+\n|        285| 1.0|\n+-----------+----+\n\n"}],"execution_count":33},{"source":"### **1205 - Monthly Transactions II**","metadata":{},"cell_type":"markdown","id":"c18add4d-780e-4494-b848-6847101f7c85"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max,datediff,lag,ifnull,curdate,count,date_format,col,avg,ceil,sum,ifnull,lit,when,to_date\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType,TimestampType, LongType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\ntransactions_data = [\n [101 ,'US'      ,'approved', 1000   ,'2019-05-18'],\n [ 102 ,'US'      ,'declined', 2000   ,'2019-05-19'],\n [ 103 ,'US'      ,'approved', 3000   ,'2019-06-10'],\n [ 104 ,'US'      ,'declined', 4000   ,'2019-06-13'],\n [ 105 ,'US'      ,'approved', 5000   ,'2019-06-15']]\n\nsch = ['id','country','state','amount','trans_date']\n\ntransactions_df = spark.createDataFrame(data=transactions_data,schema=sch)\ntransactions_df = transactions_df.withColumn('trans_date',to_date(col('trans_date'),'yyyy-MM-dd'))\ntransactions_df.show()\ntransactions_df.printSchema()\n# root\n#  |-- id: long (nullable = true)\n#  |-- country: string (nullable = true)\n#  |-- state: string (nullable = true)\n#  |-- amount: long (nullable = true)\n#  |-- trans_date: date (nullable = true)\n\nchargebacks_data =  [[102      ,'2019-05-29'],\n [ 101      ,'2019-06-30'],\n [ 105      ,'2019-09-18']]\nsch = ['trans_id','trans_date']\nchargebacks_df = spark.createDataFrame(data=chargebacks_data,schema=sch)\nchargebacks_df = chargebacks_df.withColumn('trans_date',to_date('trans_date','yyyy-MM-dd'))\nchargebacks_df.show()\nchargebacks_df.printSchema()\n\n\nchargebacks_info_df = chargebacks_df.join(transactions_df,chargebacks_df['trans_id']==transactions_df['id'])\\\n.withColumn('state',lit('chargeback'))\\\n.select('trans_id',col('country'),'state','amount',chargebacks_df['trans_date'])\n\n\nfinal_df = transactions_df.unionAll(chargebacks_info_df)\nfinal_df.show()\n\n\n# for each month and country: \n# the number of approved transactions and their total amount, \n# the number of chargebacks, and their total amount\n\nans_df = final_df.withColumn('month',date_format('trans_date','yyyy-MM'))\\\n.groupby('month','country')\\\n.agg(\n    sum(when(col('state')=='approved',1).otherwise(0)).alias('approves'),\n    sum(when(col('state')=='approved',col('amount')).otherwise(0)).alias('approved_amount'),\n    sum(when(col('state')=='chargeback',1).otherwise(0)).alias('chargebacks'),\n    sum(when(col('state')=='chargeback',col('amount')).otherwise(0)).alias('chargeback_amount')\n)\n\nans_df.show()","metadata":{"executionCancelledAt":null,"executionTime":2521,"lastExecutedAt":1722277144255,"lastExecutedByKernel":"1c22a5d7-1513-4049-ac73-4ddc906296c8","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max,datediff,lag,ifnull,curdate,count,date_format,col,avg,ceil,sum,ifnull,lit,when,to_date\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType,TimestampType, LongType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\ntransactions_data = [\n [101 ,'US'      ,'approved', 1000   ,'2019-05-18'],\n [ 102 ,'US'      ,'declined', 2000   ,'2019-05-19'],\n [ 103 ,'US'      ,'approved', 3000   ,'2019-06-10'],\n [ 104 ,'US'      ,'declined', 4000   ,'2019-06-13'],\n [ 105 ,'US'      ,'approved', 5000   ,'2019-06-15']]\n\nsch = ['id','country','state','amount','trans_date']\n\ntransactions_df = spark.createDataFrame(data=transactions_data,schema=sch)\ntransactions_df = transactions_df.withColumn('trans_date',to_date(col('trans_date'),'yyyy-MM-dd'))\ntransactions_df.show()\ntransactions_df.printSchema()\n# root\n#  |-- id: long (nullable = true)\n#  |-- country: string (nullable = true)\n#  |-- state: string (nullable = true)\n#  |-- amount: long (nullable = true)\n#  |-- trans_date: date (nullable = true)\n\nchargebacks_data =  [[102      ,'2019-05-29'],\n [ 101      ,'2019-06-30'],\n [ 105      ,'2019-09-18']]\nsch = ['trans_id','trans_date']\nchargebacks_df = spark.createDataFrame(data=chargebacks_data,schema=sch)\nchargebacks_df = chargebacks_df.withColumn('trans_date',to_date('trans_date','yyyy-MM-dd'))\nchargebacks_df.show()\nchargebacks_df.printSchema()\n\n\nchargebacks_info_df = chargebacks_df.join(transactions_df,chargebacks_df['trans_id']==transactions_df['id'])\\\n.withColumn('state',lit('chargeback'))\\\n.select('trans_id',col('country'),'state','amount',chargebacks_df['trans_date'])\n\n\nfinal_df = transactions_df.unionAll(chargebacks_info_df)\nfinal_df.show()\n\n\n# 1. each month and country: the number of approved transactions\n\nans_df = final_df.withColumn('month',date_format('trans_date','yyyy-MM'))\\\n.groupby('month','country')\\\n.agg(\n    sum(when(col('state')=='approved',1).otherwise(0)).alias('approves'),\n    sum(when(col('state')=='approved',col('amount')).otherwise(0)).alias('approved_amount'),\n    sum(when(col('state')=='chargeback',1).otherwise(0)).alias('chargebacks'),\n    sum(when(col('state')=='chargeback',col('amount')).otherwise(0)).alias('chargeback_amount')\n)\n\nans_df.show()","outputsMetadata":{"0":{"height":616,"type":"stream"},"1":{"height":227,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"6c1b66a1-5a1b-43d4-b3d8-df2915531700","outputs":[{"output_type":"stream","name":"stdout","text":"+---+-------+--------+------+----------+\n| id|country|   state|amount|trans_date|\n+---+-------+--------+------+----------+\n|101|     US|approved|  1000|2019-05-18|\n|102|     US|declined|  2000|2019-05-19|\n|103|     US|approved|  3000|2019-06-10|\n|104|     US|declined|  4000|2019-06-13|\n|105|     US|approved|  5000|2019-06-15|\n+---+-------+--------+------+----------+\n\nroot\n |-- id: long (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n |-- amount: long (nullable = true)\n |-- trans_date: date (nullable = true)\n\n+--------+----------+\n|trans_id|trans_date|\n+--------+----------+\n|     102|2019-05-29|\n|     101|2019-06-30|\n|     105|2019-09-18|\n+--------+----------+\n\nroot\n |-- trans_id: long (nullable = true)\n |-- trans_date: date (nullable = true)\n\n+---+-------+----------+------+----------+\n| id|country|     state|amount|trans_date|\n+---+-------+----------+------+----------+\n|101|     US|  approved|  1000|2019-05-18|\n|102|     US|  declined|  2000|2019-05-19|\n|103|     US|  approved|  3000|2019-06-10|\n|104|     US|  declined|  4000|2019-06-13|\n|105|     US|  approved|  5000|2019-06-15|\n|101|     US|chargeback|  1000|2019-06-30|\n|102|     US|chargeback|  2000|2019-05-29|\n|105|     US|chargeback|  5000|2019-09-18|\n+---+-------+----------+------+----------+\n\n+-------+-------+--------+---------------+-----------+-----------------+\n|  month|country|approves|approved_amount|chargebacks|chargeback_amount|\n+-------+-------+--------+---------------+-----------+-----------------+\n|2019-05|     US|       1|           1000|          1|             2000|\n|2019-06|     US|       2|           8000|          1|             1000|\n|2019-09|     US|       0|              0|          1|             5000|\n+-------+-------+--------+---------------+-----------+-----------------+\n\n"}],"execution_count":14},{"source":"### **1070. Product Sales Analysis III**","metadata":{},"cell_type":"markdown","id":"40f67e68-8290-46d1-a401-7cd367ec3859"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max,datediff,lag,ifnull,curdate,count,date_format,col,avg,ceil,sum,ifnull,lit,when,to_date,dense_rank,min\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType,TimestampType, LongType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\nsales_schema = ['sale_id','product_id','year','quantity','price']\ndata = [\n    [ 1       , 100        , 2008 , 10       , 5000],\n    [ 2       , 100        , 2009 , 12       , 5000],\n    [ 7       , 200        , 2011 , 15       , 9000],\n    [ 5       , 100        , 2008 , 10       , 5000]\n]\nsales_df = spark.createDataFrame(data=data,schema=sales_schema )\n\n# sales_df.withColumn('rnk',dense_rank().over(\n#     Window.partitionBy(col('product_id')).orderBy('year')\n# )\n#                    ).where(col('rnk')==1).dropDuplicates(['product_id']).show()\n\n\nsales_df.withColumn('rnk',\n                   dense_rank().over(Window.partitionBy(col('product_id')).orderBy('year')\n                                                       \n                                                       )\n\n                   ).where(col('rnk')==1).dropDuplicates(['product_id']).show()\n\n\n\n\n\nsales_df.groupBy(col('product_id')).agg(\n    min(col('year')).alias('firsyYr'),\n    max(col('year')).alias('lastYr')    \n                                       )\\\n                                       .show()","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":311,"type":"stream"},"1":{"height":164,"type":"stream"}}},"cell_type":"code","id":"5f0678db-152c-4d19-8e58-32ccc2b73f62","outputs":[{"output_type":"stream","name":"stdout","text":"+-------+----------+----+--------+-----+---+\n|sale_id|product_id|year|quantity|price|rnk|\n+-------+----------+----+--------+-----+---+\n|      1|       100|2008|      10| 5000|  1|\n|      7|       200|2011|      15| 9000|  1|\n+-------+----------+----+--------+-----+---+\n\n+----------+-------+------+\n|product_id|firsyYr|lastYr|\n+----------+-------+------+\n|       100|   2008|  2009|\n|       200|   2011|  2011|\n+----------+-------+------+\n\n"}],"execution_count":3},{"source":"### **1098 - Unpopular Books**","metadata":{},"cell_type":"markdown","id":"33a880a6-76e5-45a8-a88b-b3e39fd82033"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max,datediff,lag,ifnull,curdate,count,date_format,col,avg,ceil,sum,ifnull,lit,when,to_date,dense_rank\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType,TimestampType, LongType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\nbooks_schm = ['book_id','name','available_from']\nbooks_data= [\n        [1, 'Clark', '2010-01-01'],\n[2, 'Dave', '2012-05-12'],\n[3, 'Ava', '2019-06-10'],\n[4, 'Clark', '2019-06-01'],\n[5, 'Clark', '2008-09-21']\n]\n\nbooks_df = spark.createDataFrame(data=books_data,schema=books_schm)\norders_sch = ['order_id','book_id','quantity','dispatch_date']\n\n\norders_data=[\n[ 1        , 1       , 2        , '2018-07-26'    ],\n[ 2        , 1       , 1        , '2018-11-05'    ],\n[ 3        , 3       , 8        , '2019-06-11'    ],\n[ 4        , 4       , 6        , '2019-06-05'    ],\n[ 5        , 4       , 5        , '2019-06-20'    ],\n[ 6        , 5       , 9        , '2009-02-02'    ],\n[ 7        , 5       , 8        , '2019-04-13'    ]\n]\n\norders_df = spark.createDataFrame(schema=orders_sch,data=orders_data)\norders_df=orders_df.withColumn('dispatch_date',to_date(col('dispatch_date')))\norders_df.printSchema()\norders_df.join(books_df,'book_id','right')\\\n.withColumn('dispatch_date',when(col('dispatch_date').isNull(),'2019-05-23'\n                                ).otherwise(col('dispatch_date')))\\\n.withColumn('available_from',when(col('available_from').isNull(),'2019-05-23'\n                                ).otherwise(col('available_from')))\\\n.where( (date_format('dispatch_date','yyyy-MM')>='2018-05')\n       & \n       (col('available_from')<='2019-05-23')\n      )\\\n.groupby('book_id').agg(sum('quantity').alias('order').alias('cnt')).where(\n    (col('cnt').isNull()) |    (col('cnt')<=10)).show()\n\n","metadata":{"executionCancelledAt":null,"executionTime":660,"lastExecutedAt":1722365433442,"lastExecutedByKernel":"fcae74ba-fb25-4205-b1cc-c7296b78dbcb","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max,datediff,lag,ifnull,curdate,count,date_format,col,avg,ceil,sum,ifnull,lit,when,to_date,dense_rank\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType,TimestampType, LongType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\nbooks_schm = ['book_id','name','available_from']\nbooks_data= [\n        [1, 'Clark', '2010-01-01'],\n[2, 'Dave', '2012-05-12'],\n[3, 'Ava', '2019-06-10'],\n[4, 'Clark', '2019-06-01'],\n[5, 'Clark', '2008-09-21']\n]\n\nbooks_df = spark.createDataFrame(data=books_data,schema=books_schm)\norders_sch = ['order_id','book_id','quantity','dispatch_date']\n\n\norders_data=[\n[ 1        , 1       , 2        , '2018-07-26'    ],\n[ 2        , 1       , 1        , '2018-11-05'    ],\n[ 3        , 3       , 8        , '2019-06-11'    ],\n[ 4        , 4       , 6        , '2019-06-05'    ],\n[ 5        , 4       , 5        , '2019-06-20'    ],\n[ 6        , 5       , 9        , '2009-02-02'    ],\n[ 7        , 5       , 8        , '2019-04-13'    ]\n]\n\norders_df = spark.createDataFrame(schema=orders_sch,data=orders_data)\norders_df=orders_df.withColumn('dispatch_date',to_date(col('dispatch_date')))\norders_df.printSchema()\norders_df.join(books_df,'book_id','right')\\\n.withColumn('dispatch_date',when(col('dispatch_date').isNull(),'2019-05-23'\n                                ).otherwise(col('dispatch_date')))\\\n.withColumn('available_from',when(col('available_from').isNull(),'2019-05-23'\n                                ).otherwise(col('available_from')))\\\n.where( (date_format('dispatch_date','yyyy-MM')>='2018-05')\n       & \n       (col('available_from')<='2019-05-23')\n      )\\\n.groupby('book_id').agg(sum('quantity').alias('order').alias('cnt')).where(\n    (col('cnt').isNull()) |    (col('cnt')<=10)).show()\n\n","outputsMetadata":{"0":{"height":311,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":164,"type":"stream"}}},"cell_type":"code","id":"e08d42ce-16db-4ffa-b104-927aebdb465b","outputs":[{"output_type":"stream","name":"stdout","text":"root\n |-- order_id: long (nullable = true)\n |-- book_id: long (nullable = true)\n |-- quantity: long (nullable = true)\n |-- dispatch_date: date (nullable = true)\n\n+-------+----+\n|book_id| cnt|\n+-------+----+\n|      1|   3|\n|      2|NULL|\n|      5|   8|\n+-------+----+\n\n"}],"execution_count":25},{"source":"### **2228 - Users With Two Purchases Within Seven Days**","metadata":{},"cell_type":"markdown","id":"46238142-5e74-4a08-94a1-22243ef35cf9"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max,datediff,lag,ifnull,curdate,count,date_format,col,avg,ceil,sum,ifnull,lit,when,to_date,dense_rank, concat,collect_list\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType,TimestampType, LongType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\npurchases_data = [\n    [ 4           , 2       ,'2022-03-13'],\n    [ 1           , 5       ,'2022-02-11'],\n    [ 3           , 7       ,'2022-06-19'],\n    [ 6           , 2       ,'2022-03-20'],\n    [ 5           , 7       ,'2022-06-19'],\n    [ 2           , 2       ,'2022-06-08']\n]\nschema = ['purchase_id','user_id','purchase_date']\npurchase_df = spark.createDataFrame(schema=schema,data=purchases_data)\npurchase_df= purchase_df.withColumn('purchase_date',to_date('purchase_date','yyyy-MM-dd'))\ndf1 = purchase_df.alias('df1')    \ndf2 = purchase_df.alias('df2')\n# df2 = df2.withColumnRenamed('purchase_date','purchase_dateB')\n# .withColumn('diff',datediff(df1['purchase_date'],df2['purchase_dateB']))\\\ndf1.join(df2,col('df1.user_id')==col('df2.user_id'))\\\n.withColumn('diff',datediff(col('df1.purchase_date'),col('df2.purchase_date')))\\\n.where(col('diff').between(0,7))\\\n.groupBy(col('df1.user_id'),df1['purchase_date'])\\\n.agg(count('df2.purchase_date').alias('countforlast7days'),\n    collect_list(col('df2.purchase_date'))\n    )\\\n.where(col('countforlast7days')>1)\\\n.show(truncate=False)","metadata":{"executionCancelledAt":null,"executionTime":10537,"lastExecutedAt":1732046266564,"lastExecutedByKernel":"967a3fce-0f87-4506-9ea9-ab8c41d37987","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max,datediff,lag,ifnull,curdate,count,date_format,col,avg,ceil,sum,ifnull,lit,when,to_date,dense_rank, concat,collect_list\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType,TimestampType, LongType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\npurchases_data = [\n    [ 4           , 2       ,'2022-03-13'],\n    [ 1           , 5       ,'2022-02-11'],\n    [ 3           , 7       ,'2022-06-19'],\n    [ 6           , 2       ,'2022-03-20'],\n    [ 5           , 7       ,'2022-06-19'],\n    [ 2           , 2       ,'2022-06-08']\n]\nschema = ['purchase_id','user_id','purchase_date']\npurchase_df = spark.createDataFrame(schema=schema,data=purchases_data)\npurchase_df= purchase_df.withColumn('purchase_date',to_date('purchase_date','yyyy-MM-dd'))\ndf1 = purchase_df.alias('df1')    \ndf2 = purchase_df.alias('df2')\n# df2 = df2.withColumnRenamed('purchase_date','purchase_dateB')\n# .withColumn('diff',datediff(df1['purchase_date'],df2['purchase_dateB']))\\\ndf1.join(df2,col('df1.user_id')==col('df2.user_id'))\\\n.withColumn('diff',datediff(col('df1.purchase_date'),col('df2.purchase_date')))\\\n.where(col('diff').between(0,7))\\\n.groupBy(col('df1.user_id'),df1['purchase_date'])\\\n.agg(count('df2.purchase_date').alias('countforlast7days'),\n    collect_list(col('df2.purchase_date'))\n    )\\\n.where(col('countforlast7days')>1)\\\n.show(truncate=False)","outputsMetadata":{"0":{"height":122,"type":"stream"},"1":{"height":164,"type":"stream"}}},"cell_type":"code","id":"950de0c0-580b-4370-b728-489180a4b1b9","outputs":[{"output_type":"stream","name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/11/19 19:57:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n                                                                                \r"},{"output_type":"stream","name":"stdout","text":"+-------+-------------+-----------------+------------------------------------------------+\n|user_id|purchase_date|countforlast7days|collect_list(df2.purchase_date)                 |\n+-------+-------------+-----------------+------------------------------------------------+\n|2      |2022-03-20   |2                |[2022-03-13, 2022-03-20]                        |\n|7      |2022-06-19   |4                |[2022-06-19, 2022-06-19, 2022-06-19, 2022-06-19]|\n+-------+-------------+-----------------+------------------------------------------------+\n\n"}],"execution_count":1},{"source":"### ** 2978 - Symmetric Coordinates**","metadata":{},"cell_type":"markdown","id":"09860ee6-1870-46aa-b2ad-2a4d9dbe6a25"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max,datediff,lag,ifnull,curdate,count,date_format,col,avg,ceil,sum,ifnull,lit,when,to_date,dense_rank, concat,collect_list\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType,TimestampType, LongType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\ncoords_data = [\n[ 20 , 20 ],\n[ 20 , 20 ],\n[ 20 , 21 ],\n[ 23 , 22 ],\n[ 22 , 23 ],\n[ 21 , 20 ]\n]\n\n\ncoords_df = spark.createDataFrame(schema=['x','y'],data=coords_data)\ndf1 = coords_df.alias('df1')\ndf2 = coords_df.alias('df2')\n\ndf1.join(df2, (col('df1.x')==col('df2.y')) & (col('df1.y')==col('df2.x'))).groupby(df1.x,df1.y).agg(count('*'))\\\n.where(col('x')<=col('y')).show()\n# .dropDuplicates([col('x'),col('y')]).show()\n\n\n\ndf1.join(df2, (col('df1.x')==col('df2.y')) & (col('df1.y')==col('df2.x'))).select(df1.x,df1.y).drop_duplicates(['x','y'])\\\n.where(col('x')<=col('y')).show()\n\n# .dropDuplicates([col('x'),col('y')]).show()\n","metadata":{"executionCancelledAt":null,"executionTime":1014,"lastExecutedAt":1722433022797,"lastExecutedByKernel":"ef97eeb3-8db6-4981-920b-2d79e0b9a3f0","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max,datediff,lag,ifnull,curdate,count,date_format,col,avg,ceil,sum,ifnull,lit,when,to_date,dense_rank, concat,collect_list\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType,TimestampType, LongType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\ncoords_data = [\n[ 20 , 20 ],\n[ 20 , 20 ],\n[ 20 , 21 ],\n[ 23 , 22 ],\n[ 22 , 23 ],\n[ 21 , 20 ]\n]\n\n\ncoords_df = spark.createDataFrame(schema=['x','y'],data=coords_data)\ndf1 = coords_df.alias('df1')\ndf2 = coords_df.alias('df2')\n\ndf1.join(df2, (col('df1.x')==col('df2.y')) & (col('df1.y')==col('df2.x'))).groupby(df1.x,df1.y).agg(count('*'))\\\n.where(col('x')<=col('y')).show()\n# .dropDuplicates([col('x'),col('y')]).show()\n\n\n\ndf1.join(df2, (col('df1.x')==col('df2.y')) & (col('df1.y')==col('df2.x'))).select(df1.x,df1.y).drop_duplicates(['x','y'])\\\n.where(col('x')<=col('y')).show()\n\n# .dropDuplicates([col('x'),col('y')]).show()\n","outputsMetadata":{"0":{"height":353,"type":"stream"}}},"cell_type":"code","id":"a3e4b18b-4e2b-4787-aa96-d4ae443c1aa1","outputs":[{"output_type":"stream","name":"stdout","text":"+---+---+--------+\n|  x|  y|count(1)|\n+---+---+--------+\n| 20| 20|       4|\n| 20| 21|       1|\n| 22| 23|       1|\n+---+---+--------+\n\n+---+---+\n|  x|  y|\n+---+---+\n| 20| 20|\n| 20| 21|\n| 22| 23|\n+---+---+\n\n"}],"execution_count":37},{"source":"### **-- 1107 - New Users Daily Count**","metadata":{},"cell_type":"markdown","id":"b7374629-e172-4578-86f8-6bbb5fe0e79d"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import lit, expr, min,max,datediff,lag,ifnull,curdate,count,date_format,col,avg,ceil,sum,ifnull,lit,when,to_date,dense_rank, concat,collect_list\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType,TimestampType, LongType\nfrom datetime import datetime  # Import datetime module\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\ntraffic_data = [\n    [1       ,'login','2019-05-01'],\n    [1       ,'homepage','2019-05-01'],\n    [ 1       ,'logout','2019-05-01'],\n    [ 2       ,'login','2019-06-21'],\n    [ 2       ,'logout','2019-06-21'],\n    [ 3       ,'login','2019-01-01'],\n    [ 3       ,'jobs','2019-01-01'],\n    [ 3       ,'logout','2019-01-01'],\n    [ 4       ,'login','2019-06-21'],\n    [ 4       ,'groups','2019-06-21'],\n    [ 4       ,'logout','2019-06-21'],\n    [ 5       ,'login','2019-03-01'],\n    [ 5       ,'logout','2019-03-01'],\n    [ 5       ,'login','2019-06-21'],\n    [ 5       ,'logout','2019-06-21'],\n    [ 5       ,'groups','2019-06-22']\n    \n]\n\n\nsch = ['user_id','activity','activity_date']\n\ntraffic_df = spark.createDataFrame(schema=sch,data=traffic_data)\ntraffic_df = traffic_df.withColumn('activity_date',to_date(col('activity_date')))\n\nfirstLoginDF = traffic_df.where(col('activity')=='login').groupBy(col('user_id')).agg(min('activity_date').alias('FistLoginDate'))\nfirstLoginDF.show()\ndistinctDatesDF = traffic_df.withColumn('cur_date',lit('2019-06-30'))\\\n.where(\n    datediff(col('cur_date'),col('activity_date')).between(0,90))\n.dropDuplicates(['activity_date']) #to get the distinct dates\n\n\n# .distinct()\n# .select(expr('distinct activity_date'))\n\n\ndistinctDatesDF.show()\n\n\ndistinctDatesDF.join(firstLoginDF,distinctDatesDF['activity_date']==firstLoginDF['FistLoginDate'],'left')\\\n.groupBy(distinctDatesDF['activity_date']).agg(count(firstLoginDF['FistLoginDate']).alias('logins')).show()\n# count('').show()\n\n# firstLogin_df = traffic_df.where(col('activity')=='login').groupby('user_id')\\\n# .agg(min(col('activity_date')).alias('firstLoginDate'))\n\n# traffic_df.join(firstLogin_df,'user_id')\\\n# .withColumn('cur_date',lit('2019-06-30'))\\\n# .where(\n#     (col('activity')=='login') &\n#     (col('activity_date')==col('firstLoginDate')) \n#     & datediff(col('cur_date'),col('activity_date')).between(0,90)\n# )\\\n# .groupBy('activity_date').agg(count('*').alias('cnt'))\\\n# .withColumn('sumSoFar',sum('cnt').over(Window.orderBy('activity_date')))\\\n# .show()","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":122,"type":"stream"},"1":{"height":616,"type":"stream"},"2":{"height":269,"type":"stream"}}},"cell_type":"code","id":"a8e05acd-305b-4193-8a70-d90349bf52dc","outputs":[{"output_type":"stream","name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/11/20 17:10:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n                                                                                \r"},{"output_type":"stream","name":"stdout","text":"+-------+-------------+\n|user_id|FistLoginDate|\n+-------+-------------+\n|      1|   2019-05-01|\n|      3|   2019-01-01|\n|      2|   2019-06-21|\n|      5|   2019-03-01|\n|      4|   2019-06-21|\n+-------+-------------+\n\n+-------+--------+-------------+----------+\n|user_id|activity|activity_date|  cur_date|\n+-------+--------+-------------+----------+\n|      1|   login|   2019-05-01|2019-06-30|\n|      1|homepage|   2019-05-01|2019-06-30|\n|      1|  logout|   2019-05-01|2019-06-30|\n|      2|   login|   2019-06-21|2019-06-30|\n|      2|  logout|   2019-06-21|2019-06-30|\n|      4|   login|   2019-06-21|2019-06-30|\n|      4|  groups|   2019-06-21|2019-06-30|\n|      4|  logout|   2019-06-21|2019-06-30|\n|      5|   login|   2019-06-21|2019-06-30|\n|      5|  logout|   2019-06-21|2019-06-30|\n|      5|  groups|   2019-06-22|2019-06-30|\n+-------+--------+-------------+----------+\n\n+-------------+------+\n|activity_date|logins|\n+-------------+------+\n|   2019-06-21|    14|\n|   2019-05-01|     3|\n|   2019-06-22|     0|\n+-------------+------+\n\n"}],"execution_count":1},{"source":"### **177 - Nth Highest Salary**","metadata":{},"cell_type":"markdown","id":"d67d5298-b948-43a4-a8ef-b4144eb5ca21"},{"source":"from pyspark.sql import SparkSession\n# from pyspark.sql.functions import \nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType,TimestampType, LongType\n# from datetime import datetime  # Import datetime module\n# from pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\nfrom datetime import datetime\n\n\nsch = ['id','salary']\ndata = [\n    [1   ,100    ],\n    [ 2   ,200    ],\n    [ 3   ,300]\n]\nn = int(input('Enter number:'))\ndf = spark.createDataFrame(schema=sch,data=data)\ndf.dropDuplicates(['salary']).orderBy(col('salary').desc()).offset(n-1).limit(1).show()","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":164,"type":"stream"}}},"cell_type":"code","id":"17872776-f828-4b58-9b3f-600f5880e201","outputs":[{"name":"stdout","output_type":"stream","text":"Enter number:3\n+---+------+\n| id|salary|\n+---+------+\n|  1|   100|\n+---+------+\n\n"}],"execution_count":38},{"source":"### **1843 - Suspicious Bank Accounts**","metadata":{},"cell_type":"markdown","id":"0d103b8e-1094-48f3-96e4-ae64c7079a44"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,date_format,to_timestamp,months_between,lag,when,lit\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\n\ndata =[\n    [2              , 3          ,'Creditor', 107100 ,'2021-06-02 11:38:14'],\n    [ 4              , 4          ,'Creditor', 10400  ,'2021-06-20 12:39:18'],\n    [ 11             , 4          ,'Debtor', 58800  ,'2021-07-23 12:41:55'],\n    [ 1              , 4          ,'Creditor', 49300  ,'2021-05-03 16:11:04'],\n    [ 15             , 3          ,'Debtor', 75500  ,'2021-05-23 14:40:20'],\n    [ 10             , 3          ,'Creditor', 102100 ,'2021-06-15 10:37:16'],\n    [ 14             , 4          ,'Creditor', 56300  ,'2021-07-21 12:12:25'],\n    [ 19             , 4          ,'Debtor', 101100 ,'2021-05-09 15:21:49'],\n    [ 8              , 3          ,'Creditor', 64900  ,'2021-07-26 15:09:56'],\n    [ 7              , 3          ,'Creditor',  90900  ,'2021-06-14 11:23:07']\n]\n\ntrans_df = spark.createDataFrame(schema=['transaction_id','account_id','type','amount','day'],data=data)\ntrans_df=trans_df.withColumn('day',to_timestamp('day'))\n\ntrans_df.select(expr(''))\n\nincome_df=trans_df.where(col('type')=='Creditor')\\\n.withColumn('month',date_format('day','y-MM'))\\\n.groupby('account_id','month').agg(sum('amount').alias('totAmount'))\n\ndata = [[ 3,21000], [ 4, 10400] ]\nacc_df = spark.createDataFrame(data=data,schema=['account_id','max_income'])\n# acc_df.show()\n\n\nexceed_df = income_df.join(acc_df,'account_id').where(col('totAmount')>=col('max_income'))\\\n.withColumn('diff',\n           months_between(\n               'month',\n               lag('month',1,'month').over(Window.partitionBy('account_id').orderBy('month'))\n            )\n           )\n\nexceed_df.show()\nexceed_df.withColumn('diff',\n                     when(col('diff').isNull(),0).otherwise(col('diff'))\n                    )\\\n.where(col('diff')>0).show()\n","metadata":{"executionCancelledAt":null,"executionTime":1145,"lastExecutedAt":1722692968435,"lastExecutedByKernel":"83a5a646-3392-4296-9a28-b4c096a93411","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,date_format,to_timestamp,months_between,lag,when,lit\nfrom pyspark.sql.window import Window\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\n\ndata =[\n    [2              , 3          ,'Creditor', 107100 ,'2021-06-02 11:38:14'],\n    [ 4              , 4          ,'Creditor', 10400  ,'2021-06-20 12:39:18'],\n    [ 11             , 4          ,'Debtor', 58800  ,'2021-07-23 12:41:55'],\n    [ 1              , 4          ,'Creditor', 49300  ,'2021-05-03 16:11:04'],\n    [ 15             , 3          ,'Debtor', 75500  ,'2021-05-23 14:40:20'],\n    [ 10             , 3          ,'Creditor', 102100 ,'2021-06-15 10:37:16'],\n    [ 14             , 4          ,'Creditor', 56300  ,'2021-07-21 12:12:25'],\n    [ 19             , 4          ,'Debtor', 101100 ,'2021-05-09 15:21:49'],\n    [ 8              , 3          ,'Creditor', 64900  ,'2021-07-26 15:09:56'],\n    [ 7              , 3          ,'Creditor',  90900  ,'2021-06-14 11:23:07']\n]\n\ntrans_df = spark.createDataFrame(schema=['transaction_id','account_id','type','amount','day'],data=data)\ntrans_df=trans_df.withColumn('day',to_timestamp('day'))\nincome_df=trans_df.where(col('type')=='Creditor')\\\n.withColumn('month',date_format('day','y-MM'))\\\n.groupby('account_id','month').agg(sum('amount').alias('totAmount'))\n\ndata = [[ 3,21000], [ 4, 10400] ]\nacc_df = spark.createDataFrame(data=data,schema=['account_id','max_income'])\n# acc_df.show()\n\n\nexceed_df = income_df.join(acc_df,'account_id').where(col('totAmount')>=col('max_income'))\\\n.withColumn('diff',\n           months_between(\n               'month',\n               lag('month',1,'month').over(Window.partitionBy('account_id').orderBy('month'))\n            )\n           )\n\nexceed_df.show()\nexceed_df.withColumn('diff',\n                     when(col('diff').isNull(),0).otherwise(col('diff'))\n                    )\\\n.where(col('diff')>0).show()\n","outputsMetadata":{"0":{"height":395,"type":"stream"},"1":{"height":143,"type":"stream"}}},"cell_type":"code","id":"8915ff78-e89b-479f-8215-1068b99d51d6","outputs":[{"output_type":"stream","name":"stdout","text":"+----------+-------+---------+----------+----+\n|account_id|  month|totAmount|max_income|diff|\n+----------+-------+---------+----------+----+\n|         3|2021-06|   300100|     21000|NULL|\n|         3|2021-07|    64900|     21000| 1.0|\n|         4|2021-05|    49300|     10400|NULL|\n|         4|2021-06|    10400|     10400| 1.0|\n|         4|2021-07|    56300|     10400| 1.0|\n+----------+-------+---------+----------+----+\n\n+----------+-------+---------+----------+----+\n|account_id|  month|totAmount|max_income|diff|\n+----------+-------+---------+----------+----+\n|         3|2021-07|    64900|     21000| 1.0|\n|         4|2021-06|    10400|     10400| 1.0|\n|         4|2021-07|    56300|     10400| 1.0|\n+----------+-------+---------+----------+----+\n\n"}],"execution_count":38},{"source":"### **1149 - Article Views II**","metadata":{},"cell_type":"markdown","id":"b84770a9-7f3f-4b29-9a44-2708f6662dce"},{"source":"from pyspark.sql import SparkSession \nfrom pyspark.sql.functions import count,col\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\n\ndata = [\n    [ 1          , 3         , 5         ,' 2019-08-01' ],\n    [ 3          , 4         , 5         ,' 2019-08-01' ],\n    [ 1          , 3         , 6         ,' 2019-08-02' ],\n    [ 2          , 7         , 7         ,' 2019-08-01' ],\n    [ 2          , 7         , 6         ,' 2019-08-02' ],\n    [ 4          , 7         , 1         ,' 2019-07-22' ],\n    [ 3          , 4         , 4         ,' 2019-07-21' ],\n    [ 3          , 4         , 4         ,' 2019-07-21' ]\n]\nschema = ['article_id','author_id','viewer_id','view_date']\nview_df = spark.createDataFrame(data=data,schema=schema)\n\nview_df.distinct().groupby([col('viewer_id'),col('view_date')]).count().where(col('count')>1).show()\n# .agg(count(col('article_id')).alias('cnt')).where(col('cnt')>1).show()\n\n\n\n\nview_df.dropDuplicates().groupby('viewer_id','view_date').agg(count('article_id').alias('view')).where(col('view')>1).show()","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":311,"type":"stream"},"1":{"height":311,"type":"stream"}}},"cell_type":"code","id":"767a0acb-9cb2-4414-a84b-d2b79b9b6910","outputs":[{"output_type":"stream","name":"stdout","text":"+---------+-----------+-----+\n|viewer_id|  view_date|count|\n+---------+-----------+-----+\n|        6| 2019-08-02|    2|\n|        5| 2019-08-01|    2|\n+---------+-----------+-----+\n\n+---------+-----------+----+\n|viewer_id|  view_date|view|\n+---------+-----------+----+\n|        6| 2019-08-02|   2|\n|        5| 2019-08-01|   2|\n+---------+-----------+----+\n\n"}],"execution_count":5},{"source":"# **180. Consecutive Numbers**","metadata":{},"cell_type":"markdown","id":"86ddf106-88c1-4e42-93ff-5a05b066c817"},{"source":"from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\n\ndata = [\n[1  , 1],\n[ 2  , 1],\n[ 3  , 1],\n[ 4  , 2],\n[ 5  , 2],\n[ 6  , 2],\n[ 7  , 2] ]\nfrom pyspark.sql.functions import lit,when,col,lag,expr\nfrom pyspark.sql.window import Window\n\ndf = spark.createDataFrame(schema=['id','num'],data=data)\n\ndf1 = df.alias('df1')\ndf2 = df.alias('df2')\ndf3 = df.alias('df3')\n\n# df1.join(df2, \n#          (col('df1.id')+1 == col('df2.id')) & \n#          (col('df2.id')+1 == col('df3.id')) & \n#          (col('df1.num') == col('df2.num')) &\n#          (col('df2.num') == col('df3.num'))\n#         ).distinct().show()\n\ndf.withColumn('lag1',\n              lag(col('num'),1,'')\n              .over(\n                  Window.partitionBy(lit('a')).orderBy(col('id'))\n                                                )\n             ).show()\n\n\n# df.withColumn('flag',\n#               when(\n#                   (col('num')==(lag(col('num'),1,-1)\n#                     .over(\n#                         Window.partitionBy(lit('a')).orderBy(col('id'))\n#                          )))&\n#                   (col('num')==(lag(col('num'),2,-1)\n#                     .over(\n#                         Window.partitionBy(lit('a')).orderBy(col('id'))\n#                          )))\n#                   ,\n#                   True\n#                    ).otherwise(False)\n#              ).where(col('flag')==True).select(expr(\"num as ConsecutiveNums\")).show()","metadata":{"executionCancelledAt":null,"executionTime":297,"lastExecutedAt":1732216197752,"lastExecutedByKernel":"24215dc9-95b5-4da9-b8a9-fa0ef25fd61e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\n\ndata = [\n[1  , 1],\n[ 2  , 1],\n[ 3  , 1],\n[ 4  , 2],\n[ 5  , 2],\n[ 6  , 2],\n[ 7  , 2] ]\nfrom pyspark.sql.functions import lit,when,col,lag,expr\nfrom pyspark.sql.window import Window\n\ndf = spark.createDataFrame(schema=['id','num'],data=data)\n\ndf1 = df.alias('df1')\ndf2 = df.alias('df2')\ndf3 = df.alias('df3')\n\n# df1.join(df2, \n#          (col('df1.id')+1 == col('df2.id')) & \n#          (col('df2.id')+1 == col('df3.id')) & \n#          (col('df1.num') == col('df2.num')) &\n#          (col('df2.num') == col('df3.num'))\n#         ).distinct().show()\n\ndf.withColumn('lag1',\n              lag(col('num'),1,'')\n              .over(\n                  Window.partitionBy(lit('a')).orderBy(col('id'))\n                                                )\n             ).show()\n\n\n# df.withColumn('flag',\n#               when(\n#                   (col('num')==(lag(col('num'),1,-1)\n#                     .over(\n#                         Window.partitionBy(lit('a')).orderBy(col('id'))\n#                          )))&\n#                   (col('num')==(lag(col('num'),2,-1)\n#                     .over(\n#                         Window.partitionBy(lit('a')).orderBy(col('id'))\n#                          )))\n#                   ,\n#                   True\n#                    ).otherwise(False)\n#              ).where(col('flag')==True).select(expr(\"num as ConsecutiveNums\")).show()","outputsMetadata":{"0":{"height":269,"type":"stream"},"1":{"height":185,"type":"stream"}}},"cell_type":"code","id":"8d7b4e7e-ffc5-4aaa-9948-0267a50433b1","outputs":[{"output_type":"stream","name":"stdout","text":"+---+---+----+\n| id|num|lag1|\n+---+---+----+\n|  1|  1|NULL|\n|  2|  1|   1|\n|  3|  1|   1|\n|  4|  2|   1|\n|  5|  2|   2|\n|  6|  2|   2|\n|  7|  2|   2|\n+---+---+----+\n\n"}],"execution_count":21},{"source":"# Investments in 2016","metadata":{},"cell_type":"markdown","id":"a7559a59-2cd7-4810-8a46-ee266deaee57"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, IntegerType\nfrom pyspark.sql.functions import col,count,sum\n\nspark = SparkSession.builder.appName('inv2016').getOrCreate()\nprint(spark.sparkContext)\n\ndata = [\n[ 1   , 10       , 5        , 10  , 10],\n[ 2   , 20       , 20       , 20  , 20],\n[ 3   , 10       , 30       , 20  , 20],\n[ 4   , 10       , 40       , 40  , 40]\n]\n\nschema = StructType([\n    StructField('pid',IntegerType()),\n    StructField('tiv_2015',IntegerType()),\n    StructField('tiv_2016',IntegerType()),\n    StructField('lat',IntegerType()),\n    StructField('lon',IntegerType())\n])\n\n\ndf = spark.createDataFrame(data = data, schema=schema)\n\nsingleLocDF = df.groupby(col('lat'),col('lon')).agg(count(col('pid')).alias('cnt')).where(col('cnt')==1)\n\n\nuniqueLocDF  = df.join(singleLocDF,\n       (df['lon']==singleLocDF['lon']) &\n        (df['lat']==singleLocDF['lat'])\n       )\n\nmultipletiv2015DF = uniqueLocDF.groupby(col('tiv_2015')).agg(count(col('pid')).alias('cnt')).where(col('cnt')>1)\n\nuniqueLocDF.join(multipletiv2015DF,\n                 uniqueLocDF['tiv_2015']==\n                 multipletiv2015DF['tiv_2015']\n\n            ).select(sum(col('tiv_2016')).alias('cnt')).show()\n\n\n\n# sum('tiv_2016').show()\n\n\n\n# df['tiv_2015']\n# col('tiv_2015')\n\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":164,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":164,"type":"stream"}}},"cell_type":"code","id":"ecfc14fc-07b8-4fa2-bc44-6535ec9de58a","outputs":[{"output_type":"stream","name":"stdout","text":"<SparkContext master=local[*] appName=inv2016>\n+---+\n|cnt|\n+---+\n| 45|\n+---+\n\n"}],"execution_count":44},{"source":"### Restaurant Growth","metadata":{},"cell_type":"markdown","id":"2d91b349-ad77-46ed-bfd3-2e6bd0c36e72"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField,IntegerType, DateType, StringType\nfrom pyspark.sql.functions import sum\n\n\ndata = [\n[1           ,'Jhon'         ,'2019-01-01', 100],\n[ 2           ,'Daniel'       ,'2019-01-02', 110],\n[ 3           ,'Jade'         ,'2019-01-03', 120],\n[ 4           ,'Khaled'       ,'2019-01-04', 130],\n[ 5           ,'Winston'      ,'2019-01-05', 110],\n[ 6           ,'Elvis'        ,'2019-01-06', 140],\n[ 7           ,'Anna'         ,'2019-01-07', 150],\n[ 8           ,'Maria'        ,'2019-01-08', 80],\n[ 9           ,'Jaze'         ,'2019-01-09', 110],\n[ 1           ,'Jhon'         ,'2019-01-10', 130],\n[ 3           ,'Jade'         ,'2019-01-10', 150]\n]\n\n\nschema = StructType([\n        StructField('customer_id',IntegerType()),\n        StructField('name',StringType()),\n        StructField('visited_on',DateType()),\n        StructField('amount',IntegerType())\n])\n\ndf = spark.createDataFrame(data = data, schema = schema)\n\nminDateDF = df.select(min(col('visited_on')).alias('minDate'))\n\n\n\n\n\n\n\n\n","metadata":{},"cell_type":"code","id":"948281dc-0c39-4431-8647-bbff55b53b40","outputs":[],"execution_count":null},{"source":"### **1532 - The Most Recent Three Orders**","metadata":{},"cell_type":"markdown","id":"20d4c823-325c-4f69-b0c9-477b1c065cd9"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import IntegerType, StringType, DateType, StructType,StructField,StringType\nfrom datetime import datetime\nspark = SparkSession.builder.appName('1532').getOrCreate()\n\nsch = StructType(\n[\n    StructField('order_id',IntegerType()),\n    StructField('order_date',DateType()),\n    StructField('customer_id',IntegerType()),\n    StructField('cost',IntegerType()),\n]\n)\n\nconvertStrToDT = lambda x:datetime.strptime(x,'%Y-%m-%d')\n\ndata = [\n[1      ,convertStrToDT('2020-07-31'),  1  ,1   ],\n[ 2        ,convertStrToDT('2020-07-30'),  2           , 2 ],         \n[ 3        ,convertStrToDT('2020-07-31'),  3           , 3 ],         \n[ 4        ,convertStrToDT('2020-07-29'),  4           , 1 ],         \n[ 5        ,convertStrToDT('2020-06-10'),  1           , 2 ],         \n[ 6        ,convertStrToDT('2020-08-01'),  2           , 1],          \n[ 7        ,convertStrToDT('2020-08-01'),  3           , 1],          \n[ 8        ,convertStrToDT('2020-08-03'),  1           , 2],          \n[ 9        ,convertStrToDT('2020-08-07'),  2           , 3],          \n[ 10       ,convertStrToDT('2020-07-15'),  1           , 2]\n\n\n\n\n\n]\ndf = spark.createDataFrame(data= data,schema = sch)\ndf.show()\n\n\nfrom pyspark.sql.functions import col,rank\n\nwd = Window.partitionBy('customer_id').orderBy(col('order_date').desc())\n\ndf = df.withColumn('rnk',rank().over(wd)).where(col('rnk')<4)\n\n\nsch = StructType(\n[\n    StructField('customer_id',IntegerType()),\n    StructField('name',StringType())\n    \n]\n)\n\ndata = [\n[ 1           ,'Winston'],\n[ 2           , 'Jonathan'],\n[ 3           , 'Annabelle'],\n[ 4           , 'Marwan'],\n[ 5           , 'Khaled']   \n]\n\ndf2 = spark.createDataFrame(data=data,schema=sch)\n\ndf.join(df2,'customer_id','left').orderBy(col('name'),col('customer_id'),col('order_date').desc()).show()\n\n\n\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":616,"type":"stream"},"1":{"height":143,"type":"stream"}}},"cell_type":"code","id":"01cf085d-45fa-45a3-b1df-7094c5735fd4","outputs":[{"output_type":"stream","name":"stdout","text":"+--------+----------+-----------+----+\n|order_id|order_date|customer_id|cost|\n+--------+----------+-----------+----+\n|       1|2020-07-31|          1|   1|\n|       2|2020-07-30|          2|   2|\n|       3|2020-07-31|          3|   3|\n|       4|2020-07-29|          4|   1|\n|       5|2020-06-10|          1|   2|\n|       6|2020-08-01|          2|   1|\n|       7|2020-08-01|          3|   1|\n|       8|2020-08-03|          1|   2|\n|       9|2020-08-07|          2|   3|\n|      10|2020-07-15|          1|   2|\n+--------+----------+-----------+----+\n\n+-----------+--------+----------+----+---+---------+\n|customer_id|order_id|order_date|cost|rnk|     name|\n+-----------+--------+----------+----+---+---------+\n|          3|       7|2020-08-01|   1|  1|Annabelle|\n|          3|       3|2020-07-31|   3|  2|Annabelle|\n|          2|       9|2020-08-07|   3|  1| Jonathan|\n|          2|       6|2020-08-01|   1|  2| Jonathan|\n|          2|       2|2020-07-30|   2|  3| Jonathan|\n|          4|       4|2020-07-29|   1|  1|   Marwan|\n|          1|       8|2020-08-03|   2|  1|  Winston|\n|          1|       1|2020-07-31|   1|  2|  Winston|\n|          1|      10|2020-07-15|   2|  3|  Winston|\n+-----------+--------+----------+----+---+---------+\n\n"}],"execution_count":16},{"source":"# 2112 - The Airport With the Most Traffic","metadata":{},"cell_type":"markdown","id":"46af08a7-aa11-4bcd-95c0-ed4e5f14728c"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import IntegerType,StringType,StructType,StructField\n\nspark = SparkSession.builder.appName('2112').getOrCreate()\n\nsch = StructType(\n[\n    StructField('departure_airport',IntegerType()),\n    StructField('arrival_airport',IntegerType()),\n    StructField('flights_count',IntegerType())    \n    \n])\n\n\ndata = [\n    [1                 , 2               , 4  ],\n    [2                 , 1               , 5 ],\n    [ 2                 , 4               , 5 ]    \n]\n\nfrom pyspark.sql.functions import sum,col,max\ndf = spark.createDataFrame(data=data,schema=sch)\n\ndf = df.select('departure_airport','flights_count').unionAll(df.select('arrival_airport','flights_count')).groupBy('departure_airport').agg(sum(col('flights_count')).alias('totFlights'))\n\nmaxFlights = df.select(max('totFlights').alias('maxFlights'))\n\ndf.join(maxFlights, df['totFlights']==maxFlights['maxFlights'],'anti').show()\n\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":164,"type":"stream"}}},"cell_type":"code","id":"12cd7ac0-679a-469a-8e31-e7ae77d30304","outputs":[{"output_type":"stream","name":"stdout","text":"+-----------------+----------+\n|departure_airport|totFlights|\n+-----------------+----------+\n|                1|         9|\n|                4|         5|\n+-----------------+----------+\n\n"}],"execution_count":27},{"source":"# 2142. The Number of Passengers in Each Bus I","metadata":{},"cell_type":"markdown","id":"42f0f669-62e6-471a-bee9-ed8730fa75cb"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, IntegerType\n\nspark=SparkSession.builder.appName('2142').getOrCreate()\n\nsch = StructType([\n    StructField('bus_id',IntegerType()),    \n    StructField('arrival_time',IntegerType())\n    ]\n)\ndata =[[ 1      , 2            ],\n[ 2      , 4            ],\n[ 3      , 7]]\n\n\nbusDF = spark.createDataFrame(schema= sch,data=data)\n\ndata = [\n[11           , 1           ],\n[ 12           , 5            ],\n[ 13           , 6            ],\n[ 14           , 7  ]\n]\n\nsch = StructType(\n    [\n        StructField('passenger_id',IntegerType()),\n        StructField('arrival_time',IntegerType())\n    ]\n)\npassDF = spark.createDataFrame(data=data, schema=sch)\n\nfrom pyspark.sql.functions import count,avg,lag,lit,col\n\n\ndf = busDF.join(passDF,busDF['arrival_time']>=passDF['arrival_time'],'left').groupBy('bus_id').agg(count('passenger_id').alias('psgCnt'))\n\ndf = df.join(busDF,'bus_id')\n\nfrom pyspark.sql.window import Window\n\n\nwd = Window.orderBy('arrival_time')\n\ndf.withColumn('cnt',col('psgCnt') - lag(col('psgCnt'),1,0).over(wd)).drop('psgCnt').show()\n\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":395,"type":"stream"},"1":{"height":185,"type":"stream"},"2":{"height":185,"type":"stream"}}},"cell_type":"code","id":"163ff94d-a8a0-476e-b635-22d1faf162f8","outputs":[{"output_type":"stream","name":"stderr","text":"24/12/20 14:57:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/12/20 14:57:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/12/20 14:57:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/12/20 14:57:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/12/20 14:57:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/12/20 14:57:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/12/20 14:57:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/12/20 14:57:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/12/20 14:57:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"},{"output_type":"stream","name":"stdout","text":"+------+------------+---+\n|bus_id|arrival_time|cnt|\n+------+------------+---+\n|     1|           2|  1|\n|     2|           4|  0|\n|     3|           7|  3|\n+------+------------+---+\n\n"},{"output_type":"stream","name":"stderr","text":"24/12/20 14:57:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/12/20 14:57:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/12/20 14:57:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/12/20 14:57:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"}],"execution_count":22},{"source":"### 1990 - Count the Number of Experiments","metadata":{},"cell_type":"markdown","id":"6fc5d5f9-1941-4ea3-b604-19146e084181"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField,StringType\n\nspark = SparkSession.builder.appName('1990').getOrCreate()\n\nsch = StructType([\n    StructField('experiment_id',StringType()),\n    StructField('platform',StringType()),\n    StructField('experiment_name',StringType())\n])\n\ndata = [\n    [ 4             , 'IOS'      , 'Programming'     ],\n    [ 13            , 'IOS'      , 'Sports'          ],\n    [ 14            , 'Android'  , 'Reading'         ],\n    [ 8             , 'Web'      , 'Reading'         ],\n    [ 12            , 'Web'      , 'Reading'         ],\n    [ 18            , 'Web'      , 'Programming'     ]\n]\n\n\ndf = spark.createDataFrame(data=data,schema=sch)\n\nplatDF = spark.createDataFrame(schema=['plat'],data = [['IOS'],['Android'],['Web']])\nexpDF = spark.createDataFrame(data = [['Programming'],['Sports'],['Reading']],schema = ['expName'])\n\n\ndataDF = platDF.crossJoin(expDF)\nfrom pyspark.sql.functions import col,count\nansDF = dataDF.join(df, (\n    (dataDF['plat']==df['platform']) &\n    (dataDF['expName']==df['experiment_name'])\n                ),\n            'left'\n           )\\\n.groupBy(col('plat'),col('expName')).agg(\n    count(col('experiment_id'))\n                                           )\nansDF.show()\n# ,col('dataDF.platform')\n                                                      \n# .count('experimet_id').show()","metadata":{"executionCancelledAt":null,"executionTime":1528,"lastExecutedAt":1734803808052,"lastExecutedByKernel":"b9e322ed-460e-45e1-be58-a7d5afb3f74c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField,StringType\n\nspark = SparkSession.builder.appName('1990').getOrCreate()\n\nsch = StructType([\n    StructField('experiment_id',StringType()),\n    StructField('platform',StringType()),\n    StructField('experiment_name',StringType())\n])\n\ndata = [\n    [ 4             , 'IOS'      , 'Programming'     ],\n    [ 13            , 'IOS'      , 'Sports'          ],\n    [ 14            , 'Android'  , 'Reading'         ],\n    [ 8             , 'Web'      , 'Reading'         ],\n    [ 12            , 'Web'      , 'Reading'         ],\n    [ 18            , 'Web'      , 'Programming'     ]\n]\n\n\ndf = spark.createDataFrame(data=data,schema=sch)\n\nplatDF = spark.createDataFrame(schema=['plat'],data = [['IOS'],['Android'],['Web']])\nexpDF = spark.createDataFrame(data = [['Programming'],['Sports'],['Reading']],schema = ['expName'])\n\n\ndataDF = platDF.crossJoin(expDF)\nfrom pyspark.sql.functions import col,count\nansDF = dataDF.join(df, (\n    (dataDF['plat']==df['platform']) &\n    (dataDF['expName']==df['experiment_name'])\n                ),\n            'left'\n           )\\\n.groupBy(col('plat'),col('expName')).agg(\n    count(col('experiment_id'))\n                                           )\nansDF.show()\n# ,col('dataDF.platform')\n                                                      \n# .count('experimet_id').show()","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":311,"type":"stream"}}},"cell_type":"code","id":"dcac7968-71c2-4d2d-b471-affe18954b30","outputs":[{"output_type":"stream","name":"stderr","text":"                                                                                \r"},{"output_type":"stream","name":"stdout","text":"+-------+-----------+--------------------+\n|   plat|    expName|count(experiment_id)|\n+-------+-----------+--------------------+\n|    IOS|Programming|                   1|\n|    IOS|    Reading|                   0|\n|    IOS|     Sports|                   1|\n|    Web|Programming|                   1|\n|Android|Programming|                   0|\n|Android|     Sports|                   0|\n|Android|    Reading|                   1|\n|    Web|     Sports|                   0|\n|    Web|    Reading|                   2|\n+-------+-----------+--------------------+\n\n"}],"execution_count":28},{"source":"### 1501. Countries You Can Safely Invest In","metadata":{},"cell_type":"markdown","id":"9b1e83a6-2f51-4b21-83a2-01f8965beb85"},{"source":"from pyspark.sql import SparkSession\nfrom datetime import datetime\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\nspark = SparkSession.builder.appName('1501').getOrCreate()\n\n\n\ndt = lambda x: datetime.strtime(x,'%Y-%m-%d %H:%M:%S')\n\ncallSchema = StructType([\n    StructField('caller_id',IntegerType()),\n    StructField('callee_id',IntegerType()),\n    StructField('duration',IntegerType())\n])\n\npersonSchema = StructType([\n    StructField('id',IntegerType()),\n    StructField('name',StringType()),\n    StructField('phone_number',StringType())\n])\n\ncountrySchema = StructType([\n    StructField('cname',StringType()),\n    StructField('country_code',StringType())\n])\n\npersonData = [\n    [ 3  , 'Jonathan' , '051-1234567'],\n[ 12 , 'Elvis'    , '051-7654321'],\n[ 1  , 'Moncef'   , '212-1234567'],\n[ 2  , 'Maroua'   , '212-6523651'],\n[ 7  , 'Meir'     , '972-1234567'],\n[ 9  , 'Rachel'   , '972-0011100' ]\n]\n\n\ncountryData = [\n    ['Peru', '051'],\n[ 'Israel','972'],\n[ 'Morocco','212'],\n[ 'Germany','049'],\n[ 'Ethiopia','251']\n]\n\ncallsData = [\n[ 1         , 9         , 33       ],\n[2         , 9         , 4        ],\n[ 1         , 2         , 59       ],\n[ 3         , 12        , 102      ],\n[ 3         , 12        , 330      ],\n[ 12        , 3         , 5        ],\n[ 7         , 9         , 13       ],\n[ 7         , 1         , 3        ],\n[ 9         , 7         , 1        ],\n[ 1         , 7         , 7]\n]\n\ncallsDF = spark.createDataFrame(schema=callSchema,data = callsData)\npersonsDF = spark.createDataFrame(schema = personSchema,data = personData)\ncountryDF = spark.createDataFrame(schema=countrySchema,data = countryData)\n\nfrom pyspark.sql.functions import col, avg,substring,round\n\nallCallsDF = callsDF.select(col('caller_id').alias('id'),'duration')\\\n.unionAll(callsDF.select('callee_id','duration'))\n\ndataDF = personsDF.join(countryDF,\n                       substring(personsDF['phone_number'],1,3)==countryDF['country_code']\n                       )\n\njoinedDF = allCallsDF.join(dataDF,'id')\n\nglobalAvg = joinedDF.agg(avg('duration').alias('globalAvg')).collect()\nprint(globalAvg[0]['globalAvg'])\njoinedDF.groupby('cname').agg( round(avg('duration'),2).alias('avgCountryWise')).where(col('avgCountryWise')>globalAvg[0]['globalAvg']).show()\n\n\n\n\n\n\n","metadata":{"executionCancelledAt":null,"executionTime":1542,"lastExecutedAt":1735224081538,"lastExecutedByKernel":"7d8b15b2-2064-442d-9893-fa4b2ff89dcb","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom datetime import datetime\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\nspark = SparkSession.builder.appName('1501').getOrCreate()\n\n\n\ndt = lambda x: datetime.strtime(x,'%Y-%m-%d %H:%M:%S')\n\ncallSchema = StructType([\n    StructField('caller_id',IntegerType()),\n    StructField('callee_id',IntegerType()),\n    StructField('duration',IntegerType())\n])\n\npersonSchema = StructType([\n    StructField('id',IntegerType()),\n    StructField('name',StringType()),\n    StructField('phone_number',StringType())\n])\n\ncountrySchema = StructType([\n    StructField('cname',StringType()),\n    StructField('country_code',StringType())\n])\n\npersonData = [\n    [ 3  , 'Jonathan' , '051-1234567'],\n[ 12 , 'Elvis'    , '051-7654321'],\n[ 1  , 'Moncef'   , '212-1234567'],\n[ 2  , 'Maroua'   , '212-6523651'],\n[ 7  , 'Meir'     , '972-1234567'],\n[ 9  , 'Rachel'   , '972-0011100' ]\n]\n\n\ncountryData = [\n    ['Peru', '051'],\n[ 'Israel','972'],\n[ 'Morocco','212'],\n[ 'Germany','049'],\n[ 'Ethiopia','251']\n]\n\ncallsData = [\n[ 1         , 9         , 33       ],\n[2         , 9         , 4        ],\n[ 1         , 2         , 59       ],\n[ 3         , 12        , 102      ],\n[ 3         , 12        , 330      ],\n[ 12        , 3         , 5        ],\n[ 7         , 9         , 13       ],\n[ 7         , 1         , 3        ],\n[ 9         , 7         , 1        ],\n[ 1         , 7         , 7]\n]\n\ncallsDF = spark.createDataFrame(schema=callSchema,data = callsData)\npersonsDF = spark.createDataFrame(schema = personSchema,data = personData)\ncountryDF = spark.createDataFrame(schema=countrySchema,data = countryData)\n\nfrom pyspark.sql.functions import col, avg,substring,round\n\nallCallsDF = callsDF.select(col('caller_id').alias('id'),'duration')\\\n.unionAll(callsDF.select('callee_id','duration'))\n\ndataDF = personsDF.join(countryDF,\n                       substring(personsDF['phone_number'],1,3)==countryDF['country_code']\n                       )\n\njoinedDF = allCallsDF.join(dataDF,'id')\n\nglobalAvg = joinedDF.agg(avg('duration').alias('globalAvg')).collect()\nprint(globalAvg[0]['globalAvg'])\njoinedDF.groupby('cname').agg( round(avg('duration'),2).alias('avgCountryWise')).where(col('avgCountryWise')>globalAvg[0]['globalAvg']).show()\n\n\n\n\n\n\n","outputsMetadata":{"0":{"height":164,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":185,"type":"stream"},"3":{"height":38,"type":"stream"}}},"cell_type":"code","id":"ea3d0c51-ff7e-482f-ac25-c66a97ee91db","outputs":[{"output_type":"stream","name":"stdout","text":"55.7\n+-----+--------------+\n|cname|avgCountryWise|\n+-----+--------------+\n| Peru|        145.67|\n+-----+--------------+\n\n"}],"execution_count":26},{"source":"### 2986. Find Third Transaction","metadata":{},"cell_type":"markdown","id":"5c538577-26fc-4f15-bc2f-3540979fce4d"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField,IntegerType,DoubleType,TimestampType\nspark = SparkSession.builder.appName('2986').getOrCreate()\nfrom datetime import datetime\n\ndt = lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S')\ndata= [\n    [ 1       , 65.56  , dt('2023-11-18 13:49:42')],\n[ 1       , 96.0   , dt('2023-11-30 02:47:26')],\n[ 1       , 7.44   , dt('2023-11-02 12:15:23')],\n[ 1       , 49.78  , dt('2023-11-12 00:13:46')],\n[ 2       , 40.89  , dt('2023-11-21 04:39:15')],\n[ 2       , 100.44 , dt('2023-11-20 07:39:34')],\n[ 3       , 37.33  , dt('2023-11-03 06:22:02')],\n[ 3       , 13.89  , dt('2023-11-11 16:00:14')],\n[ 3       , 7.0    , dt('2023-11-29 22:32:36')]\n]\n\n\nsch = StructType([\n    StructField('id',IntegerType()),\n    StructField('spend',DoubleType()),\n    StructField('transaction_date',TimestampType())\n])\n\ndf = spark.createDataFrame(schema=sch,data=data)\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import col,lag,rank,when\nwd = Window.partitionBy('id').orderBy('transaction_date')\n\ndf.withColumn('rnk',rank().over(wd))\\\n.withColumn('flag', when(\n    (col('spend')>lag('spend',1,9999999).over(wd)) & (col('spend')>lag('spend',2,9999999).over(wd)),True\n                        ).otherwise(False)\n           ).where(\n    (col('rnk')==3) & col('flag')\n                  ).select(\n'id',\n    col('spend').alias('third_transaction_spend'),\n    col('transaction_date').alias('third_transaction_date')\n).show()\n\n\n\n\n","metadata":{"executionCancelledAt":null,"executionTime":3541,"lastExecutedAt":1735297818414,"lastExecutedByKernel":"5890c70f-faeb-4ed0-b5f0-42a294bcd9b2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField,IntegerType,DoubleType,TimestampType\nspark = SparkSession.builder.appName('2986').getOrCreate()\nfrom datetime import datetime\n\ndt = lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S')\ndata= [\n    [ 1       , 65.56  , dt('2023-11-18 13:49:42')],\n[ 1       , 96.0   , dt('2023-11-30 02:47:26')],\n[ 1       , 7.44   , dt('2023-11-02 12:15:23')],\n[ 1       , 49.78  , dt('2023-11-12 00:13:46')],\n[ 2       , 40.89  , dt('2023-11-21 04:39:15')],\n[ 2       , 100.44 , dt('2023-11-20 07:39:34')],\n[ 3       , 37.33  , dt('2023-11-03 06:22:02')],\n[ 3       , 13.89  , dt('2023-11-11 16:00:14')],\n[ 3       , 7.0    , dt('2023-11-29 22:32:36')]\n]\n\n\nsch = StructType([\n    StructField('id',IntegerType()),\n    StructField('spend',DoubleType()),\n    StructField('transaction_date',TimestampType())\n])\n\ndf = spark.createDataFrame(schema=sch,data=data)\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import col,lag,rank,when\nwd = Window.partitionBy('id').orderBy('transaction_date')\n\ndf.withColumn('rnk',rank().over(wd))\\\n.withColumn('flag', when(\n    (col('spend')>lag('spend',1,9999999).over(wd)) & (col('spend')>lag('spend',2,9999999).over(wd)),True\n                        ).otherwise(False)\n           ).where(\n    (col('rnk')==3) & col('flag')\n                  ).select(\n'id',\n    col('spend').alias('third_transaction_spend'),\n    col('transaction_date').alias('third_transaction_date')\n).show()\n\n\n\n\n","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":143,"type":"stream"}}},"cell_type":"code","id":"27bf69d4-266f-43f8-ad87-9a1fd1cdcaae","outputs":[{"output_type":"stream","name":"stderr","text":"                                                                                \r"},{"output_type":"stream","name":"stdout","text":"+---+-----------------------+----------------------+\n| id|third_transaction_spend|third_transaction_date|\n+---+-----------------------+----------------------+\n|  1|                  65.56|   2023-11-18 13:49:42|\n+---+-----------------------+----------------------+\n\n"}],"execution_count":4},{"source":"# 1555. Bank Account Summary","metadata":{},"cell_type":"markdown","id":"4d56db8b-b8e1-4196-a886-09bf3fa2d3a4"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField,StringType, IntegerType\n\nspark = SparkSession.builder.appName('1555').getOrCreate()\n\nuserData = [\n    [ 1           ,'Moustafa',      100         ],\n[ 2           ,'Jonathan',      200       ],\n[ 3          ,'Winston',       10000       ],\n[ 4           ,'Luis', 800]\n]\n\nuserSch = StructType(\n    [\n    StructField('user_id',IntegerType()),\n    StructField('user_name',StringType()),\n    StructField('credit',IntegerType())\n    ]\n)\n\ntransData = [\n    [ 1          , 1          , 3          , 400],    \n[ 2          , 3          , 2          , 500],\n[ 3          , 2          , 1          , 200 ]\n]\n\ntransSch = StructType(\n    [\n    StructField('trans_id',IntegerType()),\n    StructField('paid_by',IntegerType()),\n    StructField('paid_to',IntegerType()),\n    StructField('amount',IntegerType())\n    ]\n)\n\nuserDF = spark.createDataFrame(data=userData,schema = userSch)\nuserDF.show()\ntransDF = spark.createDataFrame(data=transData,schema = transSch)\ntransDF.show()\n\nfrom pyspark.sql.functions import col,sum,when,ifnull,lit\nbalDF = transDF.select(col('paid_by').alias('id'),(-col('amount')).alias('amount')).unionAll(\n    transDF.select(col('paid_to'),col('amount').alias('amount'))\n).groupBy('id').agg(sum('amount').alias('balance'))\n\nbalDF.show()\n\nbalDF.join(userDF,balDF['id']==userDF['user_id'],'right')\\\n.withColumn('credit', ifnull(col('balance'),lit(0))+col('credit'))\\\n.withColumn('breached',when(col('credit')<0,'Yes').otherwise('No'))\\\n.select('user_id','user_name','credit','breached').show()\n \n","metadata":{"executionCancelledAt":null,"executionTime":1621,"lastExecutedAt":1735309415630,"lastExecutedByKernel":"e3e34862-fb68-473f-9ad3-1ab2ed2aabdf","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField,StringType, IntegerType\n\nspark = SparkSession.builder.appName('1555').getOrCreate()\n\nuserData = [\n    [ 1           ,'Moustafa',      100         ],\n[ 2           ,'Jonathan',      200       ],\n[ 3          ,'Winston',       10000       ],\n[ 4           ,'Luis', 800]\n]\n\nuserSch = StructType(\n    [\n    StructField('user_id',IntegerType()),\n    StructField('user_name',StringType()),\n    StructField('credit',IntegerType())\n    ]\n)\n\ntransData = [\n    [ 1          , 1          , 3          , 400],    \n[ 2          , 3          , 2          , 500],\n[ 3          , 2          , 1          , 200 ]\n]\n\ntransSch = StructType(\n    [\n    StructField('trans_id',IntegerType()),\n    StructField('paid_by',IntegerType()),\n    StructField('paid_to',IntegerType()),\n    StructField('amount',IntegerType())\n    ]\n)\n\nuserDF = spark.createDataFrame(data=userData,schema = userSch)\nuserDF.show()\ntransDF = spark.createDataFrame(data=transData,schema = transSch)\ntransDF.show()\n\nfrom pyspark.sql.functions import col,sum,when,ifnull,lit\nbalDF = transDF.select(col('paid_by').alias('id'),(-col('amount')).alias('amount')).unionAll(\n    transDF.select(col('paid_to'),col('amount').alias('amount'))\n).groupBy('id').agg(sum('amount').alias('balance'))\n\nbalDF.show()\n\nbalDF.join(userDF,balDF['id']==userDF['user_id'],'right')\\\n.withColumn('credit', ifnull(col('balance'),lit(0))+col('credit'))\\\n.withColumn('breached',when(col('credit')<0,'Yes').otherwise('No'))\\\n.select('user_id','user_name','credit','breached').show()\n \n","outputsMetadata":{"0":{"height":616,"type":"stream"},"1":{"height":38,"type":"stream"}}},"cell_type":"code","id":"7bca8c90-c65f-46cb-9559-a3b0ce6625e1","outputs":[{"output_type":"stream","name":"stdout","text":"+-------+---------+------+\n|user_id|user_name|credit|\n+-------+---------+------+\n|      1| Moustafa|   100|\n|      2| Jonathan|   200|\n|      3|  Winston| 10000|\n|      4|     Luis|   800|\n+-------+---------+------+\n\n+--------+-------+-------+------+\n|trans_id|paid_by|paid_to|amount|\n+--------+-------+-------+------+\n|       1|      1|      3|   400|\n|       2|      3|      2|   500|\n|       3|      2|      1|   200|\n+--------+-------+-------+------+\n\n+---+-------+\n| id|balance|\n+---+-------+\n|  1|   -200|\n|  3|   -100|\n|  2|    300|\n+---+-------+\n\n+-------+---------+------+--------+\n|user_id|user_name|credit|breached|\n+-------+---------+------+--------+\n|      1| Moustafa|  -100|     Yes|\n|      2| Jonathan|   500|      No|\n|      3|  Winston|  9900|      No|\n|      4|     Luis|   800|      No|\n+-------+---------+------+--------+\n\n"}],"execution_count":28},{"source":"### 184 - Department Highest Salary","metadata":{},"cell_type":"markdown","id":"254ed205-b62c-4509-85f9-888944d7243b"},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nspark = SparkSession.builder.appName('184').getOrCreate()\n\nEmployeeSchema = StructType([\n    StructField('id',IntegerType()),\n    StructField('name',StringType()),\n    StructField('salary',IntegerType()),\n    StructField('departmentId',IntegerType()),\n])\nempData = [\n    [1  , 'Joe'   , 70000  , 1],\n[ 2  , 'Jim'   , 90000  , 1],\n[ 3  , 'Henry' , 80000  , 2],\n[ 4  , 'Sam'   , 60000  , 2],\n[ 5  , 'Max'   , 90000  , 1]\n]\n\nDepartmentSchema = StructType(\n    [\n        StructField('id',IntegerType()),\n        StructField('name',StringType())\n    ]\n)\n\ndeptData = [\n    [1   ,'IT'],\n[ 2  ,'Sales']\n]\n\nempDF = spark.createDataFrame(schema=EmployeeSchema,data = empData)\ndeptDF = spark.createDataFrame(schema=DepartmentSchema,data = deptData)\n\nfrom pyspark.sql.functions import col,rank\nfrom pyspark.sql.window import Window\n\nwd = Window.partitionBy('departmentId').orderBy(col('salary').desc())\n\nempDF.withColumnRenamed('name','empName').withColumn('rnk',rank().over(wd))\\\n.where(col('rnk')==1)\\\n.join(deptDF,empDF['departmentId']==deptDF['id'])\\\n.select(empDF['id'],'departmentId','empName','name','salary')\\\n.show()","metadata":{"executionCancelledAt":null,"executionTime":978,"lastExecutedAt":1735312674464,"lastExecutedByKernel":"448954f0-183f-49d7-a705-616d590a63f2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nspark = SparkSession.builder.appName('184').getOrCreate()\n\nEmployeeSchema = StructType([\n    StructField('id',IntegerType()),\n    StructField('name',StringType()),\n    StructField('salary',IntegerType()),\n    StructField('departmentId',IntegerType()),\n])\nempData = [\n    [1  , 'Joe'   , 70000  , 1],\n[ 2  , 'Jim'   , 90000  , 1],\n[ 3  , 'Henry' , 80000  , 2],\n[ 4  , 'Sam'   , 60000  , 2],\n[ 5  , 'Max'   , 90000  , 1]\n]\n\nDepartmentSchema = StructType(\n    [\n        StructField('id',IntegerType()),\n        StructField('name',StringType())\n    ]\n)\n\ndeptData = [\n    [1   ,'IT'],\n[ 2  ,'Sales']\n]\n\nempDF = spark.createDataFrame(schema=EmployeeSchema,data = empData)\ndeptDF = spark.createDataFrame(schema=DepartmentSchema,data = deptData)\n\nfrom pyspark.sql.functions import col,rank\nfrom pyspark.sql.window import Window\n\nwd = Window.partitionBy('departmentId').orderBy(col('salary').desc())\n\nempDF.withColumnRenamed('name','empName').withColumn('rnk',rank().over(wd))\\\n.where(col('rnk')==1)\\\n.join(deptDF,empDF['departmentId']==deptDF['id'])\\\n.select(empDF['id'],'departmentId','empName','name','salary')\\\n.show()","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":185,"type":"stream"}}},"cell_type":"code","id":"11a84407-f648-4aec-af2e-03c73415168d","outputs":[{"output_type":"stream","name":"stdout","text":"+---+------------+-------+-----+------+\n| id|departmentId|empName| name|salary|\n+---+------------+-------+-----+------+\n|  5|           1|    Max|   IT| 90000|\n|  2|           1|    Jim|   IT| 90000|\n|  3|           2|  Henry|Sales| 80000|\n+---+------------+-------+-----+------+\n\n"}],"execution_count":9}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}